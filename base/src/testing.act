
import acton.rts
import argparse
import file
import json
import logging
import term
import time

# -- assert ---------------------------------------------------------------------

def opt_str[T](s: ?T) -> str:
    return str(s) if s != None else "None"

class NotEqualError[T](AssertionError):
    a: ?T
    b: ?T

    def __init__(self, a, b, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.b = b
        self.error_message = msg if msg != None else "Expected equal values but they are non-equal."
        self.print_vals = print_vals

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            msg += " A: " + opt_str(self.a)
            msg += " B: " + opt_str(self.b)
        return msg

class EqualError[T](AssertionError):
    a: ?T
    b: ?T

    def __init__(self, a, b, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.b = b
        self.error_message = msg if msg != None else "Expected non-equal values but they are equal."
        self.print_vals = print_vals

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            msg += " A: " + opt_str(self.a)
            msg += " B: " + opt_str(self.b)
        return msg

class NotTrueError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.error_message = msg if msg != None else "Expected True but got non-True"
        self.print_vals = print_vals

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            msg += ", value: " + opt_str(self.a)
        return msg

class NotFalseError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.error_message = msg if msg != None else "Expected False but got non-False."
        self.print_vals = print_vals

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            msg += ", value: " + opt_str(self.a)
        return msg

class NotNoneError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.error_message = msg if msg != None else "Expected None but got non-None."
        self.print_vals = print_vals

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            msg += ", value: " + opt_str(self.a)
        return msg

class NoneError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.error_message = msg if msg != None else "Expected non-None but got None."
        self.print_vals = print_vals

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", value: " + opt_str(a)
        return msg

class NotInError[T,U](AssertionError):
    a: ?T
    b: ?U

    def __init__(self, a, b, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.b = b
        self.error_message = msg if msg != None else "Expected element not in container"
        self.print_vals = print_vals

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            msg += ", element: " + opt_str(self.a)
            msg += ", container: " + opt_str(self.b)
        return msg

class InError[T,U](AssertionError):
    a: ?T
    b: ?U

    def __init__(self, a, b, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.b = b
        self.error_message = msg if msg != None else "Expected element in container"
        self.print_vals = print_vals

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            msg += ", element: " + opt_str(self.a)
            msg += ", container: " + opt_str(self.b)
        return msg

class NotIsError[T](AssertionError):
    a: T
    b: T

    def __init__(self, a, b, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.b = b
        self.error_message = msg if msg != None else "Expected both objects to be the same identity (is)"
        self.print_vals = print_vals

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            msg += " A: " + str(self.a)
            msg += " B: " + str(self.b)
        return msg

class IsError[T](AssertionError):
    a: T
    b: T

    def __init__(self, a, b, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.b = b
        self.error_message = msg if msg != None else "Expected both objects to be different identities (is not)"
        self.print_vals = print_vals

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            msg += " A: " + str(self.a)
            msg += " B: " + str(self.b)
        return msg

class NotRaisesError(AssertionError):
    def __init__(self, msg: ?str=None, print_vals: bool=True):
        self.error_message = msg if msg != None else "Expected exception not raised"
        self.print_vals = print_vals

class IsInstanceError[T](AssertionError):
    a: ?T
    t: str

    def __init__(self, a, t: str, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.t = t
        self.error_message = msg if msg != None else "expected type of specific instance"
        self.print_vals = print_vals

class NotIsInstanceError[T](AssertionError):
    a: ?T
    t: str

    def __init__(self, a, t: str, msg: ?str=None, print_vals: bool=True):
        self.a = a
        self.t = t
        self.error_message = msg if msg != None else "expected type of specific instance"


def assertEqual[T(Eq)](a: ?T, b: ?T, msg: ?str, print_vals: bool=True):
    """Assert that two values are equal
    """
    if ((a == None and b != None)
        or (a != None and b == None)
        or (a != None and b != None and not (a == b))):
        raise NotEqualError(a, b, msg, print_vals)

def assertNotEqual[T(Eq)](a: ?T, b: ?T, msg: ?str, print_vals: bool=True):
    """Assert that two values are not equal
    """
    if ((a == None and b == None)
        or (a != None and b != None and a == b)):
        raise EqualError(a, b, msg, print_vals)

def assertTrue(a, msg: ?str, print_vals: bool=True):
    """Assert that the boolean evaluation of a value is True
    """
    if not bool(a):
        raise NotTrueError(a, msg, print_vals)

def assertFalse(a, msg: ?str, print_vals: bool=True):
    """Assert that the boolean evaluation of a value is False
    """
    if bool(a):
        raise NotFalseError(a, msg, print_vals)

def assertIs[T(Identity)](a: T, b: T, msg: ?str, print_vals: bool=True):
    """Assert that two values are the same identity, i.e. the same object
    """
    if not (a is b):
        raise NotIsError(a, b, msg)

def assertIsNot[T(Identity)](a: T, b: T, msg: ?str, print_vals: bool=True):
    """Assert that two values are not the same identity, i.e. different objects
    """
    if a is b:
        raise IsError(a, b, msg)

def assertNone(a, msg: ?str, print_vals: bool=True):
    """Assert that a value equals None
    """
    if not (a == None):
        raise NotNoneError(a, msg, print_vals)

def assertNotNone(a, msg: ?str, print_vals: bool=True):
    """Assert that a value does not equal None
    """
    if not (a != None):
        raise NoneError(a, msg, print_vals)

def assertIn(a, b, msg: ?str, print_vals: bool=True):
    """Assert that a value is in a container
    """
    if not (a in b):
        raise NotInError(a, b, msg, print_vals)

def assertNotIn(a, b, msg: ?str, print_vals: bool=True):
    """Assert that a value is not in a container
    """
    if a in b:
        raise InError(a, b, msg, print_vals)

def error(msg: ?str):
    """Raise a generic test error"""
    raise AssertionError(msg if msg != None else "Test error")

# -------------------------------------------------------------------------------

def eq_opt[T(Eq)](a: ?T, b: ?T) -> bool:
    return a != None and b != None and a == b or a == None and b == None

class TestLogger(logging.Logger):
    pass

class SyncT(value):
    def __init__(self, log_handler: logging.Handler):
        self.log_handler = log_handler

class AsyncT(value):
    def __init__(self, report_result: action(?bool, ?Exception, ?str) -> None, log_handler: logging.Handler):
        self.report_result = report_result
        self.log_handler = log_handler

    def success(self, output: ?str=None):
        self.report_result(True, None, output)

    def failure(self, exception: Exception):
        self.report_result(False, exception, None)

    def error(self, exception: Exception):
        self.report_result(None, exception, None)

class EnvT(value):
    def __init__(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        self.report_result = report_result
        self.env = env
        self.log_handler = log_handler

    def success(self, output: ?str=None):
        self.report_result(True, None, output)

    def failure(self, exception: Exception):
        self.report_result(False, exception, None)

    def error(self, exception: Exception):
        self.report_result(None, exception, None)


class Test(object):
    module: str
    name: str
    desc: str

    def __init__(self, name: str, desc: str, module):
        self.name = name
        self.desc = desc
        self.module = module

    def run(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        if isinstance(self, UnitTest):
            self.run_test(report_result, env, log_handler)
        elif isinstance(self, SimpleSyncTest):
            self.run_test(report_result, env, log_handler)
        elif isinstance(self, SyncTest):
            self.run_test(report_result, env, log_handler)
        elif isinstance(self, AsyncTest):
            self.run_test(report_result, env, log_handler)
        elif isinstance(self, EnvTest):
            self.run_test(report_result, env, log_handler)
        else:
            raise ValueError("Test: Invalid test type")

    def to_json(self):
        return {
            "module": self.module,
            "name": self.name,
            "desc": self.desc,
        }

    @staticmethod
    def from_json(data: dict[str, ?value]):
        module = data["module"]
        name = data["name"]
        desc = data["desc"]
        if isinstance(module, str) and isinstance(name, str) and isinstance(desc, str):
            return Test(name, desc, module)
        raise ValueError("Test: Invalid JSON")

def safe_hash(h: int) -> int: # TODO: Remove when https://github.com/actonlang/acton/issues/1348 is fixed
    limit = 2**63-1
    sign = 1
    if h < 0:
        limit = 2**63
        sign = -1
        h = -h

    while h > limit:
        f = h // limit
        h %= limit
        h += f

    return sign * h

extension Test(Hashable):
    def __eq__(self, other: Test) -> bool:
        return self.module == other.module and self.name == other.name

    def __hash__(self) -> int:
        #return hash(self.data)
        return safe_hash(2*hash(self.module) + 3*hash(self.name))

class UnitTest(Test):
    def __init__(self, fn: mut() -> None, name: str, desc: str, module: str):
        self.fn = fn
        self.name = name
        self.desc = desc
        self.module = module

    def run_test(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        output = None
        success = None
        exception = None
        try:
            output = self.fn()
            success = True
            exception = None
        except AssertionError as e:
            success = False
            exception = e
        except Exception as e:
            success = None
            exception = e
        report_result(success, exception, output)

class SimpleSyncTest(Test):
    def __init__(self, fn: proc() -> None, name: str, desc: str, module: str):
        self.fn = fn
        self.name = name
        self.desc = desc
        self.module = module

    def run_test(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        output = None
        success = None
        exception = None
        try:
            output = self.fn()
            success = True
            exception = None
        except AssertionError as e:
            success = False
            exception = e
        except Exception as e:
            success = None
            exception = e
        report_result(success, exception, output)

class SyncTest(Test):
    def __init__(self, fn: proc(SyncT) -> None, name: str, desc: str, module: str):
        self.fn = fn
        self.name = name
        self.desc = desc
        self.module = module

    def run_test(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        output = None
        success = None
        exception = None
        try:
            output = self.fn(SyncT(log_handler))
            success = True
            exception = None
        except AssertionError as e:
            success = False
            exception = e
        except Exception as e:
            success = None
            exception = e
        report_result(success, exception, output)

class AsyncTest(Test):
    def __init__(self, fn: proc(AsyncT) -> None, name: str, desc: str, module: str):
        self.fn = fn
        self.name = name
        self.desc = desc
        self.module = module

    def run_test(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        self.fn(AsyncT(report_result, log_handler))

class EnvTest(Test):
    def __init__(self, fn: proc(EnvT) -> None, name: str, desc: str, module: str):
        self.fn = fn
        self.name = name
        self.desc = desc
        self.module = module

    def run_test(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        self.fn(EnvT(report_result, env, log_handler))


class TestResult(object):
    """
    There are three possible outcomes for a test:
    - success: the test ran to completion with the expected results
      - for unit tests & synchronous actor tests, it means the function returned
      - for asynchronous actor & env tests, the report_result callback was called with TestResult(success=True, exception=None)
    - failure: the test encountered an unexpected value
      - for unit tests & synchronous actor tests, an AssertionError (or child thereof) was raiesd
      - for asynchronous actor & env tests, the report_result callback was called with TestResult(success=False, exception=AssertionError)
    - error: the test was unable to run to completion, encountering some other error in test setup or similar
      - for unit tests & synchronous actor tests, an Exception (or child thereof) was raised, but not an AssertionError
      - for asynchronous actor & env tests, the report_result callback was called with TestResult(success=None, exception=AssertionError)
    """
    success: ?bool
    exception: ?str
    output: ?str
    duration: float
    mem_usage_delta: int
    non_gc_mem_usage_delta: int

    def __init__(self, success: ?bool, exception: ?str, output: ?str, duration: float, mem_usage_delta: int, non_gc_mem_usage_delta: int):
        self.success = success
        self.exception = exception
        self.output = output
        self.duration = duration
        self.mem_usage_delta = mem_usage_delta
        self.non_gc_mem_usage_delta = non_gc_mem_usage_delta

    def to_json(self):
        return {
            "success": self.success,
            "exception": self.exception,
            "output": self.output,
            "duration": self.duration,
            "mem_usage_delta": self.mem_usage_delta,
            "non_gc_mem_usage_delta": self.non_gc_mem_usage_delta,
        }

    @staticmethod
    def from_json(data: dict[str, str]) -> TestResult:
        success = data["success"]
        exception = data["exception"]
        output = data["output"]
        duration = data["duration"]
        mem_usage_delta = data["mem_usage_delta"]
        non_gc_mem_usage_delta = data["non_gc_mem_usage_delta"]
        if (isinstance(success, bool)
            and isinstance(exception, str)
            and isinstance(output, str)
            and isinstance(duration, float)
            and isinstance(mem_usage_delta, int)
            and isinstance(non_gc_mem_usage_delta, int)
            ):
            return TestResult(success, exception, output, duration, mem_usage_delta, non_gc_mem_usage_delta)
        raise ValueError("Invalid TestResult JSON")


class TestInfo(object):
    definition: Test
    complete: bool
    success: ?bool
    exception: ?str
    output: ?str
    std_out: ?str
    std_err: ?str
    flaky_output: bool
    flaky: bool
    leaky: bool
    min_duration: float
    max_duration: float
    avg_duration: float
    total_duration: float
    test_duration: float
    num_iterations: int
    num_failures: int
    num_errors: int
    mem_usage_delta_avg: int
    non_gc_mem_usage_delta_avg: int
    non_gc_mem_inc_count: int
    results: list[TestResult]

    def __init__(self,
                 definition: Test,
                 complete: bool=False,
                 success: ?bool=None,
                 exception: ?str=None,
                 output: ?str=None,
                 std_out: ?str=None,
                 std_err: ?str=None,
                 flaky: bool=False,
                 min_duration: float=-1.0,
                 max_duration: float=-1.0,
                 avg_duration: float=-1.0,
                 total_duration: float=0.0,
                 test_duration: float=0.0,
                 num_iterations: int=0,
                 num_failures: int=0,
                 num_errors: int=0,
                 mem_usage_delta_avg: int=0,
                 non_gc_mem_usage_delta_avg: int=0,
                 non_gc_mem_inc_count: int=0,
                 results: list[TestResult]=[]):
        self.definition = definition
        self.complete = complete
        self.success = success
        self.exception = exception
        self.output = output
        self.std_out = std_out
        self.std_err = std_err
        self.flaky_output = False
        self.flaky = flaky
        self.leaky = False
        self.min_duration = min_duration
        self.max_duration = max_duration
        self.avg_duration = avg_duration
        self.total_duration = total_duration
        self.test_duration = test_duration
        self.num_iterations = num_iterations
        self.num_failures = num_failures
        self.num_errors = num_errors
        self.mem_usage_delta_avg = mem_usage_delta_avg
        self.non_gc_mem_usage_delta_avg = non_gc_mem_usage_delta_avg
        self.non_gc_mem_inc_count = non_gc_mem_inc_count
        self.results = results

    def update(self, complete, result: TestResult, test_duration: float=-1.0):
        self.complete = complete

        if len(self.results) == 0:
            # First result
            self.output = result.output
            self.exception = result.exception

        self.results.append(result)

        if not eq_opt(self.output, result.output):
            self.flaky_output = True

        if test_duration > 0.0:
            self.test_duration = test_duration

        self.flaky = False
        self.leaky = False
        self.min_duration = -1.0
        self.max_duration = -1.0
        self.avg_duration = -1.0
        self.total_duration = 0.0
        self.num_iterations = len(self.results)
        self.num_failures = 0
        self.num_errors = 0
        self.mem_usage_delta_avg = 0
        self.non_gc_mem_usage_delta_avg = 0
        self.non_gc_mem_inc_count = 0
        for result in self.results:
            res_success = result.success
            if res_success == False:
                self.num_failures += 1
            elif res_success == None:
                self.num_errors += 1

        for result in self.results[1:]:
            if result.duration < self.min_duration or self.min_duration < 0.0:
                self.min_duration = result.duration
            if result.duration > self.max_duration or self.max_duration < 0.0:
                self.max_duration = result.duration
            self.total_duration += result.duration

            self.mem_usage_delta_avg += result.mem_usage_delta
            self.non_gc_mem_usage_delta_avg += result.non_gc_mem_usage_delta
            if result.non_gc_mem_usage_delta > 0:
                self.non_gc_mem_inc_count += 1

        if self.num_iterations > 0:
            self.mem_usage_delta_avg = self.mem_usage_delta_avg // self.num_iterations-1
            self.non_gc_mem_usage_delta_avg = self.non_gc_mem_usage_delta_avg // self.num_iterations-1
            self.avg_duration = self.total_duration / float(self.num_iterations)

        if self.non_gc_mem_inc_count > min_def([1, self.num_iterations // 2],1) and self.non_gc_mem_usage_delta_avg > 0:
            self.leaky = True

        if self.num_failures > 0:
            self.success = False
        elif self.num_errors > 0:
            self.success = None
        else:
            self.success = True

        if (self.num_failures == 0 and self.num_errors == 0) or self.num_failures == self.num_iterations or self.num_errors == self.num_iterations:
            self.flaky = False
        else:
            self.flaky = True

    def diff(self, old: ?TestInfo) -> (min_duration: str,
                                       max_duration: str,
                                       avg_duration: str,
                                       mem_usage_delta_avg: str,
                                       non_gc_mem_usage_delta_avg: str,
                                       non_gc_mem_inc_count: str):
        def fmt_diff(new, old, unit="") -> str:
            if old != None:
                diff = float(new) - float(old)
                pct_diff = diff / float(old) * 100
                sign = "+" if diff > 0 else ""
                color = term.green if diff < 0 else term.red
                diff_str = "%s%.2f%%" % (sign, pct_diff)
                return color + "%-10s" % diff_str + term.normal
            else:
                return "%10s" % ""

        #return (min_duration=fmt_diff(self.min_duration, old.min_duration if old != None else None, "ms"))
        return (
            fmt_diff(self.min_duration, old.min_duration if old != None else None, "ms"),
            fmt_diff(self.max_duration, old.max_duration if old != None else None, "ms"),
            fmt_diff(self.avg_duration, old.avg_duration if old != None else None, "ms"),
            fmt_diff(self.mem_usage_delta_avg, old.mem_usage_delta_avg if old != None else None, "B"),
            fmt_diff(self.non_gc_mem_usage_delta_avg, old.non_gc_mem_usage_delta_avg if old != None else None, "B"),
            fmt_diff(self.non_gc_mem_inc_count, old.non_gc_mem_inc_count if old != None else None)
        )

    def to_json(self, include_results: bool=False):
        test_results = []
        if include_results:
            for r in self.results:
                test_results.append(r.to_json())

        return {
            "definition": self.definition.to_json(),
            "complete": self.complete,
            "success": self.success,
            "exception": self.exception,
            "output": self.output,
            "std_out": self.std_out,
            "std_err": self.std_err,
            "flaky": self.flaky,
            "min_duration": self.min_duration,
            "max_duration": self.max_duration,
            "avg_duration": self.avg_duration,
            "total_duration": self.total_duration,
            "test_duration": self.test_duration,
            "num_iterations": self.num_iterations,
            "num_failures": self.num_failures,
            "num_errors": self.num_errors,
            "mem_usage_delta_avg": self.mem_usage_delta_avg,
            "non_gc_mem_usage_delta_avg": self.non_gc_mem_usage_delta_avg,
            "non_gc_mem_inc_count": self.non_gc_mem_inc_count,
            "results": test_results
        }

    @staticmethod
    def from_json(json_data: dict[str, ?value]) -> TestInfo:
        def take_definition(jd: dict[str, ?value]) -> Test:
            if "definition" in jd:
                test_definition = jd["definition"]
                if isinstance(test_definition, dict):
                    return Test.from_json(test_definition)
            raise ValueError("Invalid Test JSON: " + str(jd))
        definition: Test = take_definition(json_data)
        complete = json_data["complete"]
        suc = json_data["success"]
        success: ?bool = None
        if suc != None and isinstance(suc, bool):
            success = suc
        exc = json_data["exception"]
        exception: ?str = None
        if exc != None and isinstance(exc, str):
            exception = exc
        out = json_data["output"]
        output: ?str = None
        if out != None and isinstance(out, str):
            output = out
        std_out_d = json_data["std_out"]
        std_out: ?str = None
        if std_out_d != None and isinstance(std_out_d, str):
            std_out = std_out_d
        std_err_d = json_data["std_err"]
        std_err: ?str = None
        if std_err_d != None and isinstance(std_err_d, str):
            std_err = std_err_d
        flaky = json_data["flaky"]
        min_duration = json_data["min_duration"]
        max_duration = json_data["max_duration"]
        avg_duration = json_data["avg_duration"]
        total_duration = json_data["total_duration"]
        test_duration = json_data["test_duration"]
        num_iterations = json_data["num_iterations"]
        num_failures = json_data["num_failures"]
        num_errors = json_data["num_errors"]
        mem_usage_delta_avg = json_data["mem_usage_delta_avg"]
        non_gc_mem_usage_delta_avg = json_data["non_gc_mem_usage_delta_avg"]
        non_gc_mem_inc_count = json_data["non_gc_mem_inc_count"]
        results: list[TestResult] = []
        # We don't support results in the JSON, since we don't want it for
        # performance reasons but for very generic JSON serialization support it
        # would be correct to support it.
        if (isinstance(complete, bool)
            and isinstance(flaky, bool)
            and isinstance(min_duration, float)
            and isinstance(max_duration, float)
            and isinstance(avg_duration, float)
            and isinstance(total_duration, float)
            and isinstance(test_duration, float)
            and isinstance(num_iterations, int)
            and isinstance(num_failures, int)
            and isinstance(num_errors, int)
            and isinstance(mem_usage_delta_avg, int)
            and isinstance(non_gc_mem_usage_delta_avg, int)
            and isinstance(non_gc_mem_inc_count, int)
            ):
            return TestInfo(definition,
                            complete,
                            success,
                            exception,
                            output,
                            std_out,
                            std_err,
                            flaky,
                            min_duration,
                            max_duration,
                            avg_duration,
                            total_duration,
                            test_duration,
                            num_iterations,
                            num_failures,
                            num_errors,
                            mem_usage_delta_avg,
                            non_gc_mem_usage_delta_avg,
                            non_gc_mem_inc_count,
                            results)
        else:
            raise ValueError("Invalid TestInfo JSON")

class TestRunnerConfig(object):
    perf_mode: bool
    min_test_duration: float
    output_enabled: bool

    def __init__(self, perf_mode: bool, args):
        self.perf_mode = perf_mode
        mtd = 0.05
        try:
            mtd = float(args.get_int("min_time")) / 1000.0
        except:
            pass
        self.output_enabled = False if args.get_bool("no_output") else True
        self.min_test_duration = mtd

class TimeoutError(Exception):
    pass

actor TestExecutor(syscap, config, t: Test, report_complete, env):
    """The actual executor of tests
    """
    log_handler = logging.Handler()
    log_handler.add_sink(logging.ConsoleSink())
    fcap = file.FileCap(env.cap)
    rfcap = file.ReadFileCap(fcap)
    fs = file.FS(fcap)
    var test_sw = time.Stopwatch()
    var last_report = time.Stopwatch()
    var test_info = None
    var iteration = 0

    def get_expected(module: str, test: str) -> ?str:
        filename = file.join_path([fs.cwd(), "test", "golden", module, test])
        try:
            exp_file = file.ReadFile(rfcap, filename)
            exp_data = exp_file.read().decode()
            return exp_data
        except:
            return None

    action def _report_result(test: Test, sw, non_gc_mem_usage_before, gc_total_bytes_start, gc_time_start, success: ?bool, exception: ?Exception, val: ?str):
        full_dur = sw.elapsed().to_float() * 1000.0
        gc_time_end = acton.rts.get_gc_time(syscap).total
        gc_dur = float(gc_time_end - gc_time_start)
        testiter_dur = full_dur - gc_dur
        gc_total_bytes_end = int(acton.rts.get_gc_total_bytes(syscap))
        mem_usage_delta = gc_total_bytes_end - gc_total_bytes_start
        test_dur = test_sw.elapsed().to_float()
        if config.perf_mode:
            acton.rts.gc(syscap)
            acton.rts.gc(syscap)
        non_gc_mem_usage_after = int(acton.rts.get_rss(syscap) - acton.rts.get_mem_usage(syscap))
        non_gc_mem_usage_delta = non_gc_mem_usage_after - non_gc_mem_usage_before
        #print("non-GC memory before: %d  after: %d  delta: %d" % (non_gc_mem_usage_before, non_gc_mem_usage_after, non_gc_mem_usage_delta))
        complete = True if test_dur > config.min_test_duration else False
        if test_info != None:
            exc = str(exception) if exception != None else None
            test_info.update(complete, TestResult(success, exc, val, testiter_dur, mem_usage_delta, non_gc_mem_usage_delta), test_dur*1000.0)
        if last_report.elapsed().to_float() > 0.05 or complete:
            if test_info != None and config.output_enabled:
                print("\n" + json.encode({"test_info": test_info.to_json()}), err=True)
            last_report.reset()
        if not complete:
            _run_fn(test)
        else:
            report_complete(t)

    def _run_fn(t: Test):
        print("\n== Running test, iteration:", iteration)
        print("\n== Running test, iteration:", iteration, err=True)
        # Run GC to get accurate memory usage
        if config.perf_mode:
            acton.rts.gc(syscap)
            acton.rts.gc(syscap)
        non_gc_mem_usage_before = int(acton.rts.get_rss(syscap) - acton.rts.get_mem_usage(syscap))
        gc_total_bytes_start = int(acton.rts.get_gc_total_bytes(syscap))
        gc_time_start = acton.rts.get_gc_time(syscap).total
        sw = time.Stopwatch()

        def repres(s: ?bool, e: ?Exception, val: ?str) -> None:
            # Compare expected golden value
            if val != None:
                exp_val = get_expected(t.module, t.name)
                if exp_val == None or exp_val != None and val != exp_val:
                    exc = NotEqualError(val, exp_val, "Test output does not match expected golden value.\nActual  : %s\nExpected: %s" % (val, exp_val if exp_val != None else "None"), print_vals=False)
                    _report_result(t, sw, non_gc_mem_usage_before, gc_total_bytes_start, gc_time_start, False, exc, val)
                    return
            _report_result(t, sw, non_gc_mem_usage_before, gc_total_bytes_start, gc_time_start, s, e, val)

        try:
            t.run(repres, env, log_handler)
        except AssertionError as e:
            _report_result(t, sw, non_gc_mem_usage_before, gc_total_bytes_start, gc_time_start, False, e, None)
        except Exception as e:
            _report_result(t, sw, non_gc_mem_usage_before, gc_total_bytes_start, gc_time_start, None, e, None)
        iteration += 1

    def _run_test():
        """Get the next available test and run it"""
        test_sw = time.Stopwatch()
        test_info = TestInfo(t)
        _run_fn(t)

    after 0: _run_test()


class ProjectTestResults(object):
    results: dict[str, dict[str, TestInfo]]
    last_results: dict[str, dict[str, TestInfo]]
    sw: time.Stopwatch
    expected_modules: set[str]
    printed_lines: int
    up_to_date: bool
    perf_mode: bool

    def __init__(self, perf_data: str, perf_mode: bool=False):
        self.results = {}
        self.last_results = self.parse_perf_data(perf_data)
        self.sw = time.Stopwatch()
        self.expected_modules = set([''])
        self.printed_lines = 0
        self.up_to_date = False
        self.perf_mode = perf_mode

    def parse_perf_data(self, perf_data: str) -> dict[str, dict[str, TestInfo]]:
        """Parse performance data from a file
        """
        try:
            pd = json.decode(perf_data)
            res = {}
            for modname, modres in pd.items():
                resmodres = {}
                if isinstance(modname, str) and isinstance(modres, dict):
                    for tname, tinfo in modres.items():
                        resmodres[tname] = TestInfo.from_json(tinfo)
                res[modname] = resmodres
            return res
        except:
            print("Failed to parse performance data")
            return {}

    def update(self, module_name: str, test_name: str, test_info: TestInfo):
        """Update result for individual test
        """
        self.up_to_date = False
        if module_name not in self.results:
            self.results[module_name] = {}
        self.results[module_name][test_name] = test_info

    def num_tests(self):
        cnt = 0
        for module_name in self.results:
            cnt += len(self.results[module_name])
        return cnt

    def skip_show(self):
        if len(self.expected_modules) > 0 and set(self.results.keys()) != self.expected_modules:
            return True
        return False

    def is_test_done(self, modname, name):
        if modname in self.results and name in self.results[modname]:
            test_info = self.results[modname][name]
            return test_info.complete
        return False

    def is_module_done(self, modname):
        if modname in self.results:
            for tname, test_info in self.results[modname].items():
                if not test_info.complete:
                    return False
        return True

    def to_json(self):
        """Return JSON encoded results that can be saved to a file
        """
        # TODO: rewrite using comprehensions
        res = {}
        for modname in self.results:
            modres = {}
            for tname, tinfo in self.results[modname].items():
                modres[tname] = tinfo.to_json()
            res[modname] = modres
        return json.encode(res)

    def show(self, only_show_complete=False, perf_mem=True):

        def format_diff(new, old, unit=""):
            diff = float(new) - float(old)
            pct_diff = diff / float(old) * 100
            sign = "+" if diff > 0 else ""
            color = term.green if diff < 0 else term.red
            diff_str = "%s%.2f%%" % (sign, pct_diff)
            return color + "%-10s" % diff_str + term.normal

        def format_timing(module_name, test_name):
            tinfo = self.results[module_name][test_name]

            min_dur = "--"
            avg_dur = "--"
            max_dur = "--"
            min_dur_diff = "--"
            avg_dur_diff = "--"
            max_dur_diff = "--"
            if tinfo.num_iterations > 0:
                min_dur = "%6.2f" % tinfo.min_duration + term.grey13 + "ms" + term.normal
                avg_dur = "%6.2f" % tinfo.avg_duration + term.grey13 + "ms" + term.normal
                max_dur = "%6.2f" % tinfo.max_duration + term.grey13 + "ms" + term.normal
                min_dur_diff = ""
                avg_dur_diff = ""
                max_dur_diff = ""

                try:
                    last_tinfo = self.last_results[module_name][test_name]
                    min_dur_diff = format_diff(tinfo.min_duration, last_tinfo.min_duration)
                    avg_dur_diff = format_diff(tinfo.avg_duration, last_tinfo.avg_duration)
                    max_dur_diff = format_diff(tinfo.max_duration, last_tinfo.max_duration)
                except:
                    pass

            return "%8s %10s %sAvg:%s %8s %10s %8s %10s" % (min_dur, min_dur_diff, term.grey18, term.normal, avg_dur, avg_dur_diff, max_dur, max_dur_diff)

        if self.skip_show() or self.up_to_date:
            return
        self.up_to_date = True

        errors = 0
        failures = 0
        complete = True
        if set(self.results.keys()) != self.expected_modules:
            complete = False
        for module_name in self.results:
            for test_name, test_info in self.results[module_name].items():
                if not test_info.complete:
                    complete = False
        if only_show_complete and not complete:
            return

        for i in range(self.printed_lines):
            print(term.clearline + term.up() + term.clearline, end="")
        self.printed_lines = 0
        tname_width = 20
        for modname in self.results:
            for tname, tinfo in self.results[modname].items():
                tname_width = max_def([tname_width, len(tinfo.definition.name)],0)
        tname_width += 5

        for module_name in sorted(self.results):
            if module_name == "":
                print("\nTests")
                self.printed_lines += 2
            elif len(self.results[module_name]) > 0:
                print("\nTests - module %s:" % module_name)
                self.printed_lines += 2

            for test_name, tinfo in self.results[module_name].items():
                last_tinfo = None
                if module_name in self.last_results and test_name in self.last_results[module_name]:
                    last_tinfo = self.last_results[module_name][test_name]
                prefix = "  " + tinfo.definition.name + ": "
                prefix += " " * (tname_width - len(prefix))
                success = tinfo.success
                exc = tinfo.exception
                status = ""
                msg = ""
                run_info = ""
                if tinfo.complete:
                    if exc != None:
                        msg += term.bold + term.red
                        if tinfo.flaky:
                            msg += "FLAKY "
                        if tinfo.num_errors > 0:
                            msg += "ERR"
                            errors += 1
                            run_info += "%d errors" % tinfo.num_errors
                        if tinfo.num_errors > 0 and tinfo.num_failures > 0:
                            msg += "/"
                            run_info += " and "
                        if tinfo.num_failures > 0:
                            msg += "FAIL"
                            failures += 1
                            run_info += "%d failures" % tinfo.num_failures
                        if tinfo.num_errors > 0 or tinfo.num_failures > 0:
                            run_info += " out of "
                    else:
                        msg += term.green + "OK" + term.normal
                    msg += ": "
                    if self.perf_mode:
                        msg += format_timing(module_name, test_name) + " "
                    msg += "%4d runs in %3.3fms" % (tinfo.num_iterations, tinfo.test_duration) + term.normal
                else:
                    msg = term.yellow + "**" + term.normal

                l = prefix + msg
                print(l)
                self.printed_lines += 1
                if self.perf_mode and perf_mem:
                    indent = " " * 10
                    diffs = tinfo.diff(last_tinfo)
                    print(indent + "Memory usag      : %6dKB %s" % (tinfo.mem_usage_delta_avg // 1024, diffs.mem_usage_delta_avg))
                    print(indent + "non-GC usage     : %6dKB %s" % (tinfo.non_gc_mem_usage_delta_avg // 1024, diffs.non_gc_mem_usage_delta_avg))
                    print(indent + "non-GC usage > 0 : %d%% (%d out of %d)" % ((tinfo.non_gc_mem_inc_count * 100 // tinfo.num_iterations) if tinfo.num_iterations > 0 else 0,
                                                                        tinfo.non_gc_mem_inc_count,
                                                                        tinfo.num_iterations))
                    self.printed_lines += 3
                if tinfo.complete:
                    if exc != None:
                        for line in str(exc).splitlines(None):
                            print(term.red + "    %s" % (line) + term.normal)
                            self.printed_lines += 1
                    # Print std_out/std_err if the test failed
                    if tinfo.num_failures > 0 or tinfo.num_errors > 0:
                        def test_output(msgs: str):
                            for line in msgs.splitlines():
                                if not (line.startswith("== Running test,") or line == ""):
                                    return True
                            return False

                        def dedup(buf: str) -> dict[str, set[int]]:
                            # Initialize result dictionary
                            result = {}

                            # Split buffer into individual test iterations
                            iterations = buf.split("== Running test, iteration: ")

                            # Clean up iterations - remove empty entries
                            cleaned_iterations = []
                            for it in iterations:
                                if it.strip():
                                    cleaned_iterations.append(it.strip())

                            # Process each iteration
                            for iteration in cleaned_iterations:
                                # Extract iteration number and content
                                lines = iteration.split('\n', 1)
                                iteration_num = int(lines[0])
                                content = ""
                                if len(lines) > 1: # might not be any real test output yet!?
                                    content = lines[1].strip()

                                # Add to result dictionary
                                if content not in result:
                                    result[content] = set()
                                result[content].add(iteration_num)

                            return result

                        std_out = tinfo.std_out
                        if std_out != None and test_output(std_out):
                            print("    STDOUT:")
                            self.printed_lines += 1
                            chunks = dedup(std_out.strip())
                            for chunk, test_runs in chunks.items():
                                if len(chunks) == 1:
                                    pass
                                else:
                                    print("      == %d test runs with this output:" % len(test_runs))
                                    self.printed_lines += 1
                                for line in chunk.splitlines():
                                    print("      " + line)
                                    self.printed_lines += 1
                                print("")
                                self.printed_lines += 1
                        std_err = tinfo.std_err
                        if std_err != None and test_output(std_err):
                            print("    STDERR:")
                            self.printed_lines += 1
                            chunks = dedup(std_err.strip())
                            for chunk, test_runs in chunks.items():
                                if len(chunks) == 1:
                                    pass
                                else:
                                    print("      == %d test runs with this output:" % len(test_runs))
                                    self.printed_lines += 1
                                for line in chunk.splitlines():
                                    print("      " + line)
                                    self.printed_lines += 1
                                print("")
                                self.printed_lines += 1

        print("")
        if complete:
            if self.num_tests() == 0:
                print("Nothing to test")
                print()
                return 0
            elif errors > 0 and failures > 0:
                print(term.bold + term.red + "%d error and %d failure out of %d tests (%ss)" % (errors, failures, self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 2
            elif errors > 0:
                print(term.bold + term.red + "%d out of %d tests errored (%ss)" % (errors, self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 2
            elif failures > 0:
                print(term.bold + term.red + "%d out of %d tests failed (%ss)" % (failures, self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 1
            else:
                print(term.green + "All %d tests passed (%ss)" % (self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 0
        else:
            print("Running tests...")
            print()
        self.printed_lines += 3


actor test_runner(env: Env,
                  unit_tests: dict[str, UnitTest],
                  simple_sync_tests: dict[str, SimpleSyncTest],
                  sync_tests: dict[str, SyncTest],
                  async_tests: dict[str, AsyncTest],
                  env_tests: dict[str, EnvTest]):
    sw = time.Stopwatch()
    var all_tests = {}

    for name, t in unit_tests.items():
        all_tests[name] = t
    for name, t in simple_sync_tests.items():
        all_tests[name] = t
    for name, t in sync_tests.items():
        all_tests[name] = t
    for name, t in async_tests.items():
        all_tests[name] = t
    for name, t in env_tests.items():
        all_tests[name] = t

    proc def _cmd_list(args):
        res = {}
        for test_name, test_def in all_tests.items():
            test_info = TestInfo(test_def)
            res[test_def.name] = test_info.to_json()
        print("\n" + json.encode({"tests": res}), err=True)
        env.exit(0)



    proc def _run_tests(args, perf_mode: bool=False):
        config = TestRunnerConfig(perf_mode, args)
        if config.perf_mode:
            acton.rts.start_gc_performance_measurement(env.syscap)

        test_concurrency = 1 if config.perf_mode else env.nr_wthreads
        test_timeout = 3.0

        test_name = args.get_str("name")
        test_fun_name = "_test_" + test_name
        if test_fun_name not in all_tests:
            print("Test not found: %s" % test_name)
            env.exit(1)
        the_test = all_tests[test_fun_name]
        tests_complete = set()

        def _check_timeout(test_def):
            if test_def in tests_complete:
                return
            time_s = test_timeout * 1000.0
            test_info = TestInfo(
                test_def,
                complete=True,
                success=None,
                exception="Test timeout",
                min_duration=time_s,
                max_duration=time_s,
                avg_duration=time_s,
                total_duration=time_s,
                test_duration=time_s,
                num_iterations=1,
                num_failures=0,
                num_errors=1
                )
            tests_complete.add(test_def)
            print("\n" + json.encode({"test_info": test_info.to_json()}), err=True)
            env.exit(0)

        def report_complete(t):
            if t in tests_complete:
                # Report after we hit timeout
                return
            tests_complete.add(t)
            env.exit(0)

        test_executors = []
        te = TestExecutor(env.syscap, config, the_test, report_complete, env)
        after test_timeout: _check_timeout(the_test)

    proc def _run_perf_tests(args):
        _run_tests(args, perf_mode=True)

    def _parse_args():
        p = argparse.Parser()
        p.add_bool("json", "Output results as JSON")
        p.add_bool("no_output", "No result output")
        lp = p.add_cmd("list", "list tests", _cmd_list)
        tp = p.add_cmd("test", "Run tests", _run_tests)
        tp.add_arg("name", "Name of the test to run")
        pp = tp.add_cmd("perf", "Performance benchmark tests", _run_perf_tests)
        pp.add_option("iterations", "int", "?", 1, "Number of iterations to run")
        pp.add_option("min_time", "int", "?", 1000, "Minimum time to run a test in milliseconds")

        args = p.parse(env.argv)
        _cmd = args.cmd
        if _cmd != None:
            _cmd(args)
        else:
            env.exit(0)
    try:
        _parse_args()
    except argparse.PrintUsage as exc:
        print(exc.error_message)
        env.exit(0)
    except argparse.ArgumentError as exc:
        print(exc.error_message)
        env.exit(1)
