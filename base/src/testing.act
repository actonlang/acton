
import acton.rts
import argparse
import json
import logging
import term
import time

# -- assert ---------------------------------------------------------------------

# TODO: add actual values as args to assert functions
# TODO: add __str__ to assert exceptions
class NotEqualError[T](AssertionError):
    a: ?T
    b: ?T

    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "Expected equal values but they are non-equal."
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += " A: " + str(a) if a is not None else "None"
            b = self.b
            msg += " B: " + str(b) if b is not None else "None"
        return msg

class EqualError[T](AssertionError):
    a: ?T
    b: ?T

    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "Expected non-equal values but they are equal."
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += " A: " + str(a) if a is not None else "None"
            b = self.b
            msg += " B: " + str(b) if b is not None else "None"
        return msg

class NotTrueError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None):
        self.a = a
        self.error_message = msg if msg is not None else "Expected True but got non-True"
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", value: " + str(a) if a is not None else "None"
        return msg

class NotFalseError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None):
        self.a = a
        self.error_message = msg if msg is not None else "Expected False but got non-False."
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", value: " + str(a) if a is not None else "None"
        return msg

class NotNoneError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None):
        self.a = a
        self.error_message = msg if msg is not None else "Expected None but got non-None."
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", value: " + str(a) if a is not None else "None"
        return msg

class NoneError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None):
        self.a = a
        self.error_message = msg if msg is not None else "Expected non-None but got None."
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", value: " + str(a) if a is not None else "None"
        return msg

class NotInError[T,U](AssertionError):
    a: ?T
    b: ?U

    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "Expected element not in container"
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", element: " + str(a) if a is not None else "None"
            b = self.b
            msg += ", container: " + str(b) if b is not None else "None"
        return msg

class InError[T,U](AssertionError):
    a: ?T
    b: ?U

    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "Expected element in container"
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", element: " + str(a) if a is not None else "None"
            b = self.b
            msg += ", container: " + str(b) if b is not None else "None"
        return msg

class NotIsError[T(Identity)](AssertionError):
    a: ?T
    b: ?T
    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "a is not b"

class IsError[T(Identity)](AssertionError):
    a: ?T
    b: ?T
    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "a is b"

class NotRaisesError(AssertionError):
    def __init__(self, msg):
        self.error_message = msg
        self.a = None

class IsInstanceError(AssertionError):
    def __init__(self, msg):
        self.error_message = msg
        self.a = None
        self.b = None

class NotIsInstanceError(AssertionError):
    def __init__(self, msg):
        self.error_message = msg
        self.a = None
        self.b = None


def assertEqual[T(Eq)](a: ?T, b: ?T, msg: ?str):
    if a is not None and b is not None and not (a == b):
        raise NotEqualError(a, b, msg)

def assertNotEqual(a, b, msg: ?str):
    if not (a != b):
        raise EqualError(a, b, msg)

def assertTrue(a, msg: ?str):
    if not bool(a):
        raise NotTrueError(a, msg)

def assertFalse(a, msg: ?str):
    if bool(a):
        raise NotFalseError(a, msg)

# We cannot test raises right now because we need better support for taking a
# function and its arguments as parameters or run as a context manager
# TODO: assertRaises
# TODO: assertRaisesWithMessage
# TODO: assertRaisesWithMessageRegex

# TODO: fix this
#def assertIs[T(Identity)](a: ?T, b: ?T, msg: ?str):
#    if not (a is b):
#        raise NotIsError(a, b, msg)
# TODO: fix this
#def assertIsNot[T(Identity)](a: ?T, b: ?T, msg: ?str):
#    if not (a is not b):
#        raise IsError(a, b, msg)

def assertIsNone(a, msg: ?str):
    if not (a is None):
        raise NotNoneError(a, msg)

def assertIsNotNone(a, msg: ?str):
    if not (a is not None):
        raise NoneError(a, msg)

def assertIn(a, b, msg: ?str):
    if not (a in b):
        raise NotInError(a, b, msg)

def assertNotIn(a, b, msg: ?str):
    if a in b:
        raise InError(a, b, msg)

# TODO: fix this?
#def assertIsInstance(a, b, msg: str):
#    if not isinstance(a, b):
#        assert_msg = "Expected instance of " + b + " but got non-instance"
#        if msg != "":
#            assert_msg += ": " + msg
#        raise NotIsInstanceError(assert_msg)
#
#def assertNotIsInstance(a, b, msg: str):
#    if isinstance(a, b):
#        assert_msg = "Expected not instance of " + b + " but got instance"
#        if msg != "":
#            assert_msg += ": " + msg
#        raise IsInstanceError(assert_msg)


# -------------------------------------------------------------------------------

class TestLogger(logging.Logger):
    pass

class Test(object):
    name: str
    desc: str

    def __init__(self, name: str, desc: str):
        self.name = name
        self.desc = desc

    def to_json(self):
        return {
            "name": self.name,
            "desc": self.desc,
        }

    @staticmethod
    def from_json(data: dict[str, ?value]):
        name = data["name"]
        desc = data["desc"]
        if isinstance(name, str) and isinstance(desc, str):
            return Test(name, desc)
        raise ValueError("Test: Invalid JSON")

class UnitTest(Test):
    def __init__(self, fn: mut() -> None, name: str, desc: str):
        self.fn = fn
        self.name = name
        self.desc = desc

class SyncActorTest(Test):
    def __init__(self, fn: proc(logging.Handler) -> None, name: str, desc: str):
        self.fn = fn
        self.name = name
        self.desc = desc

class AsyncActorTest(Test):
    def __init__(self, fn: proc(action(?bool, ?Exception) -> None, logging.Handler) -> None, name: str, desc: str):
        self.fn = fn
        self.name = name
        self.desc = desc

class EnvTest(Test):
    def __init__(self, fn: proc(action(?bool, ?Exception) -> None, Env, logging.Handler) -> None, name: str, desc: str):
        self.fn = fn
        self.name = name
        self.desc = desc


class TestResult(object):
    """
    There are three possible outcomes for a test:
    - success: the test ran to completion with the expected results
      - for unit tests & synchronous actor tests, it means it returned `None`
      - for asynchronous actor & env tests, the report_result callback was called with TestResult(success=True, exception=None)
    - failure: the test encountered an unexpected value
      - for unit tests & synchronous actor tests, an AssertionError (or child thereof) was raiesd
      - for asynchronous actor & env tests, the report_result callback was called with TestResult(success=False, exception=AssertionError)
    - error: the test was unable to run to completion, encountering some other error in test setup or similar
      - for unit tests & synchronous actor tests, an Exception (or child thereof) was raised, but not an AssertionError
      - for asynchronous actor & env tests, the report_result callback was called with TestResult(success=None, exception=AssertionError)
    """
    success: ?bool
    exception: ?str
    duration: float
    mem_usage_delta: int
    rss_delta: int

    def __init__(self, success: ?bool, exception: ?str, duration: float, mem_usage_delta: int, rss_delta: int):
        self.success = success
        self.exception = exception
        self.duration = duration
        self.mem_usage_delta = mem_usage_delta
        self.rss_delta = rss_delta

    def to_json(self):
        return {
            "success": self.success,
            "exception": self.exception,
            "duration": self.duration,
            "mem_usage_delta": self.mem_usage_delta,
            "rss_delta": self.rss_delta,
        }

    @staticmethod
    def from_json(data: dict[str, str]) -> TestResult:
        success = data["success"]
        exception = data["exception"]
        duration = data["duration"]
        mem_usage_delta = data["mem_usage_delta"]
        rss_delta = data["rss_delta"]
        if (isinstance(success, bool)
            and isinstance(exception, str)
            and isinstance(duration, float)
            and isinstance(mem_usage_delta, int)
            and isinstance(rss_delta, int)
            ):
            return TestResult(success, exception, duration, mem_usage_delta, rss_delta)
        raise ValueError("Invalid TestResult JSON")


class TestInfo(object):
    definition: Test
    complete: bool
    success: ?bool
    exception: ?str
    flaky: bool
    leaky: bool
    min_duration: float
    max_duration: float
    avg_duration: float
    total_duration: float
    test_duration: float
    num_iterations: int
    num_failures: int
    num_errors: int
    mem_usage_delta_avg: int
    rss_delta_avg: int
    rss_delta_positive_count: int
    results: list[TestResult]

    def __init__(self,
                 definition: Test,
                 complete: bool=False,
                 success: ?bool=None,
                 exception: ?str=None,
                 flaky: bool=False,
                 min_duration: float=-1.0,
                 max_duration: float=-1.0,
                 avg_duration: float=-1.0,
                 total_duration: float=0.0,
                 test_duration: float=0.0,
                 num_iterations: int=0,
                 num_failures: int=0,
                 num_errors: int=0,
                 results: list[TestResult]=[]):
        self.definition = definition
        self.complete = complete
        self.success = success
        self.exception = exception
        self.flaky = flaky
        self.leaky = False
        self.min_duration = min_duration
        self.max_duration = max_duration
        self.avg_duration = avg_duration
        self.total_duration = total_duration
        self.test_duration = test_duration
        self.num_iterations = num_iterations
        self.num_failures = num_failures
        self.num_errors = num_errors
        self.mem_usage_delta_avg = 0
        self.rss_delta_avg = 0
        self.rss_delta_positive_count = 0
        self.results = results

    def update(self, complete, result: TestResult, test_duration: float=-1.0):
        self.complete = complete
        self.results.append(result)
        exc = result.exception
        if exc is not None:
            self.exception = exc

        self.flaky = False
        self.leaky = False
        self.min_duration = -1.0
        self.max_duration = -1.0
        self.avg_duration = -1.0
        self.total_duration = 0.0
        if test_duration > 0.0:
            self.test_duration = test_duration
        self.num_iterations = len(self.results)
        self.num_failures = 0
        self.num_errors = 0
        self.mem_usage_delta_avg = 0
        self.rss_delta_avg = 0
        self.rss_delta_positive_count = 0
        for result in self.results:
            res_success = result.success
            if res_success == False:
                self.num_failures += 1
            elif res_success is None:
                self.num_errors += 1

            if result.duration < self.min_duration or self.min_duration < 0.0:
                self.min_duration = result.duration
            if result.duration > self.max_duration or self.max_duration < 0.0:
                self.max_duration = result.duration
            self.total_duration += result.duration

            self.mem_usage_delta_avg += result.mem_usage_delta
            self.rss_delta_avg += result.rss_delta
            if result.rss_delta > 0:
                self.rss_delta_positive_count += 1

        self.mem_usage_delta_avg = self.mem_usage_delta_avg // self.num_iterations
        self.rss_delta_avg = self.rss_delta_avg // self.num_iterations
        self.avg_duration = self.total_duration / float(self.num_iterations)

        if self.num_failures > 0:
            self.success = False
        elif self.num_errors > 0:
            self.success = None
        else:
            self.success = True

        if (self.num_failures == 0 and self.num_errors == 0) or self.num_failures == self.num_iterations or self.num_errors == self.num_iterations:
            self.flaky = False
        else:
            self.flaky = True

        #
        if self.rss_delta_positive_count > min([1, self.num_iterations // 2]) and self.rss_delta_avg > 0:
            self.leaky = True

    def to_json(self, include_results: bool=False):
        test_results = []
        if include_results:
            for r in self.results:
                test_results.append(r.to_json())

        return {
            "definition": self.definition.to_json(),
            "complete": self.complete,
            "success": self.success,
            "exception": self.exception,
            "flaky": self.flaky,
            "min_duration": self.min_duration,
            "max_duration": self.max_duration,
            "avg_duration": self.avg_duration,
            "total_duration": self.total_duration,
            "test_duration": self.test_duration,
            "num_iterations": self.num_iterations,
            "num_failures": self.num_failures,
            "num_errors": self.num_errors,
            "results": test_results
        }

    @staticmethod
    def from_json(json_data: dict[str, ?value]) -> TestInfo:
        def take_definition(jd: dict[str, ?value]) -> Test:
            if "definition" in jd:
                test_definition = jd["definition"]
                if isinstance(test_definition, dict):
                    return Test.from_json(test_definition)
            raise ValueError("Invalid Test JSON")
        definition: Test = take_definition(json_data)
        complete = json_data["complete"]
        suc = json_data["success"]
        success: ?bool = None
        if suc is not None and isinstance(suc, bool):
            success = suc
        exc = json_data["exception"]
        exception: ?str = None
        if exc is not None and isinstance(exc, str):
            exception = exc
        flaky = json_data["flaky"]
        min_duration = json_data["min_duration"]
        max_duration = json_data["max_duration"]
        avg_duration = json_data["avg_duration"]
        total_duration = json_data["total_duration"]
        test_duration = json_data["test_duration"]
        num_iterations = json_data["num_iterations"]
        num_failures = json_data["num_failures"]
        num_errors = json_data["num_errors"]
        results: list[TestResult] = []
        json_data_results = json_data["results"]
        if isinstance(json_data_results, list):
            # TODO: Use TestResult.from_json() instead
            for r in json_data_results:
                r_success = r["success"]
                r_exception = r["exception"]
                r_duration = r["duration"]
                r_mem_usage_delta = r["mem_usage_delta"]
                r_rss_delta = r["rss_delta"]
                if (isinstance(r_success, bool)
                    and isinstance(r_exception, str)
                    and isinstance(r_duration, float)
                    and isinstance(r_mem_usage_delta, int)
                    and isinstance(r_rss_delta, int)
                    ):
                    results.append(TestResult(r_success, r_exception, r_duration, r_mem_usage_delta, r_rss_delta))
                else:
                    raise ValueError("Invalid TestResult JSON")
        if (isinstance(complete, bool)
            and isinstance(flaky, bool)
            and isinstance(min_duration, float)
            and isinstance(max_duration, float)
            and isinstance(avg_duration, float)
            and isinstance(total_duration, float)
            and isinstance(test_duration, float)
            and isinstance(num_iterations, int)
            and isinstance(num_failures, int)
            and isinstance(num_errors, int)
            ):
            return TestInfo(definition, complete, success, exception, flaky, min_duration, max_duration, avg_duration, total_duration, test_duration, num_iterations, num_failures, num_errors, results)
        else:
            raise ValueError("Invalid TestInfo JSON")

class TestRunnerConfig(object):
    perf_mode: bool
    min_test_duration: float
    output_enabled: bool

    def __init__(self, perf_mode: bool, args):
        self.perf_mode = perf_mode
        mtd = 0.05
        try:
            mtd = float(args.get_int("min_time")) / 1000.0
        except:
            pass
        self.output_enabled = False if args.get_bool("no_output") else True
        self.min_test_duration = mtd

actor unit_test_runner(syscap, config, get_test, report_complete):
    """Test runner for unit tests
    """
    def _run_fn(t):
        f = t.fn
        # Run GC to get accurate memory usage
        acton.rts.gc(syscap)
        rss_before = int(acton.rts.get_rss(syscap))
        gc_total_bytes_start = int(acton.rts.get_gc_total_bytes(syscap))
        gc_time_start = acton.rts.get_gc_time(syscap).total
        sw = time.Stopwatch()
        try:
            f()
            success = True
            exception = None
        except AssertionError as e:
            success = False
            exception = str(e)
        except Exception as e:
            success = None
            exception = str(e)
        full_dur = sw.elapsed().to_float() * 1000.0
        gc_time_end = acton.rts.get_gc_time(syscap).total
        gc_dur = float(gc_time_end - gc_time_start)
        testiter_dur = full_dur - gc_dur
        gc_total_bytes_end = int(acton.rts.get_gc_total_bytes(syscap))
        mem_usage_delta = gc_total_bytes_end - gc_total_bytes_start
        rss_after = int(acton.rts.get_rss(syscap))
        rss_delta = rss_after - rss_before
        return TestResult(success, exception, testiter_dur, mem_usage_delta, rss_delta)

    def _run():
        while True:
            try:
                t = get_test()
                test_info = TestInfo(t)
                f = t.fn
                test_sw = time.Stopwatch()
                last_report = time.Stopwatch()
                complete = False
                gctotb_before = acton.rts.get_gc_total_bytes(syscap)
                while True:
                    test_result = _run_fn(t)
                    if test_sw.elapsed().to_float() > config.min_test_duration:
                        complete = True
                    test_info.update(complete, test_result, test_sw.elapsed().to_float() * 1000.0)
                    if (last_report.elapsed().to_float() > 0.05 or complete) and config.output_enabled:
                        gctotb_after = acton.rts.get_gc_total_bytes(syscap)
                        print(json.encode({"test_info": test_info.to_json()}), stderr=True)
                        last_report.reset()
                    if complete:
                        report_complete(t.name)
                        break
            except Exception as e:
                return None

    after 0: _run()


# TODO: collapse this and the above unit_test_runner, since both are sync tests
# and the only difference is the type of test, so I think this should be
# possible
# TODO: investigate if we can turn this into async style, which would open up
# for collapsing this with the async_actor_test_runner and env_test_runner
actor sync_actor_test_runner(syscap, config, get_test, report_complete):
    """Test runner for sync actor tests
    """
    log_handler = logging.Handler("TestRunner")

    def _run_fn(f):
        rss_before = int(acton.rts.get_rss(syscap))
        gc_total_bytes_start = int(acton.rts.get_gc_total_bytes(syscap))
        gc_time_start = acton.rts.get_gc_time(syscap).total
        sw = time.Stopwatch()
        success = None   # TODO: remove, but compiler complains
        exception = None # TODO: remove, but compiler complains
        try:
            f(log_handler)
            success = True
            exception = None
        except AssertionError as e:
            success = False
            exception = str(e)
        except Exception as e:
            success = None
            exception = str(e)
        full_dur = sw.elapsed().to_float() * 1000.0
        gc_time_end = acton.rts.get_gc_time(syscap).total
        gc_dur = float(gc_time_end - gc_time_start)
        testiter_dur = full_dur - gc_dur
        gc_total_bytes_end = int(acton.rts.get_gc_total_bytes(syscap))
        mem_usage_delta = gc_total_bytes_end - gc_total_bytes_start
        rss_after = int(acton.rts.get_rss(syscap))
        rss_delta = rss_after - rss_before
        return TestResult(success, exception, testiter_dur, mem_usage_delta, rss_delta)

    def _run():
        while True:
            try:
                t = get_test()
                test_info = TestInfo(t)
                f = t.fn
                test_sw = time.Stopwatch()
                last_report = time.Stopwatch()
                complete = False
                for i in range(999999):
                    test_result = _run_fn(f)
                    if test_sw.elapsed().to_float() > config.min_test_duration:
                        complete = True
                    test_info.update(complete, test_result, test_sw.elapsed().to_float() * 1000.0)
                    if (last_report.elapsed().to_float() > 0.05 or complete) and config.output_enabled:
                        print(json.encode({"test_info": test_info.to_json()}), stderr=True)
                        last_report.reset()
                    if complete:
                        report_complete(t.name)
                        break
            except Exception as e:
                return None

    after 0: _run()


# TODO: add a timeout to this
actor async_actor_test_runner(syscap, config, get_test, report_complete):
    """Test runner for async actor tests
    """
    log_handler = logging.Handler("TestRunner")
    var test_sw = time.Stopwatch()
    var last_report = time.Stopwatch()
    var test_info = None

    action def _report_result(test: AsyncActorTest, sw, rss_before, gc_total_bytes_start, gc_time_start, success: ?bool, exception: ?Exception):
        full_dur = sw.elapsed().to_float() * 1000.0
        gc_time_end = acton.rts.get_gc_time(syscap).total
        gc_dur = float(gc_time_end - gc_time_start)
        testiter_dur = full_dur - gc_dur
        gc_total_bytes_end = int(acton.rts.get_gc_total_bytes(syscap))
        mem_usage_delta = gc_total_bytes_end - gc_total_bytes_start
        rss_after = int(acton.rts.get_rss(syscap))
        rss_delta = rss_after - rss_before
        test_dur = test_sw.elapsed().to_float()
        complete = True if test_dur > config.min_test_duration else False
        if test_info is not None:
            exc = str(exception) if exception is not None else None
            test_info.update(complete, TestResult(success, exc, testiter_dur, mem_usage_delta, rss_delta), test_dur*1000.0)
        if last_report.elapsed().to_float() > 0.05 or complete:
            if test_info is not None and config.output_enabled:
                print(json.encode({"test_info": test_info.to_json()}), stderr=True)
            last_report.reset()
        if not complete:
            _run_fn(test)
        else:
            report_complete(test.name)
            _run_next()

    def _run_fn(t):
        f = t.fn
        rss_before = int(acton.rts.get_rss(syscap))
        gc_total_bytes_start = int(acton.rts.get_gc_total_bytes(syscap))
        gc_time_start = acton.rts.get_gc_time(syscap).total
        sw = time.Stopwatch()
        try:
            f(lambda s, e: _report_result(t, sw, rss_before, gc_total_bytes_start, gc_time_start, s, e), log_handler)
        except AssertionError as e:
            _report_result(t, sw, rss_before, gc_total_bytes_start, gc_time_start, False, e)
        except Exception as e:
            _report_result(t, sw, rss_before, gc_total_bytes_start, gc_time_start, None, e)

    def _run_next():
        """Get the next available test and run it"""
        test_sw = time.Stopwatch()
        try:
            t = get_test()
            test_info = TestInfo(t)
            _run_fn(t)
        except Exception as e:
            return None

    after 0: _run_next()


actor env_test_runner(syscap, config, get_test, report_complete, env):
    """Test runner for async actor tests
    """
    log_handler = logging.Handler("TestRunner")
    var test_sw = time.Stopwatch()
    var last_report = time.Stopwatch()
    var test_info = None

    action def _report_result(test: EnvTest, sw, rss_before, gc_total_bytes_start, gc_time_start, success: ?bool, exception: ?Exception):
        full_dur = sw.elapsed().to_float() * 1000.0
        gc_time_end = acton.rts.get_gc_time(syscap).total
        gc_dur = float(gc_time_end - gc_time_start)
        testiter_dur = full_dur - gc_dur
        gc_total_bytes_end = int(acton.rts.get_gc_total_bytes(syscap))
        mem_usage_delta = gc_total_bytes_end - gc_total_bytes_start
        rss_after = int(acton.rts.get_rss(syscap))
        rss_delta = rss_after - rss_before
        test_dur = test_sw.elapsed().to_float()
        complete = True if test_dur > config.min_test_duration else False
        if test_info is not None:
            exc = str(exception) if exception is not None else None
            test_info.update(complete, TestResult(success, exc, testiter_dur, mem_usage_delta, rss_delta), test_dur*1000.0)
        if last_report.elapsed().to_float() > 0.05 or complete:
            if test_info is not None and config.output_enabled:
                print(json.encode({"test_info": test_info.to_json()}), stderr=True)
            last_report.reset()
        if not complete:
            _run_fn(test)
        else:
            report_complete(test.name)
            _run_next()

    def _run_fn(t):
        f = t.fn
        rss_before = int(acton.rts.get_rss(syscap))
        gc_total_bytes_start = int(acton.rts.get_gc_total_bytes(syscap))
        gc_time_start = acton.rts.get_gc_time(syscap).total
        sw = time.Stopwatch()
        try:
            f(lambda s, e: _report_result(t, sw, rss_before, gc_total_bytes_start, gc_time_start, s, e), env, log_handler)
        except AssertionError as e:
            _report_result(t, sw, rss_before, gc_total_bytes_start, gc_time_start, False, e)
        except Exception as e:
            _report_result(t, sw, rss_before, gc_total_bytes_start, gc_time_start, None, e)

    def _run_next():
        """Get the next available test and run it"""
        test_sw = time.Stopwatch()
        try:
            t = get_test()
            test_info = TestInfo(t)
            _run_fn(t)
        except Exception as e:
            return None

    after 0: _run_next()

class ModuleError(object):
    exit_code: int
    term_signal: int
    errout: str

    def __init__(self, exit_code: int, term_signal: int, err: str):
        self.exit_code = exit_code
        self.term_signal = term_signal
        self.err = err

class ProjectTestResults(object):
    results: dict[str, dict[str, TestInfo]]
    module_error: dict[str, ModuleError]
    last_results: dict[str, dict[str, TestInfo]]
    sw: time.Stopwatch
    expected_modules: set[str]
    printed_lines: int
    up_to_date: bool
    perf_mode: bool

    def __init__(self, perf_data: str, perf_mode: bool=False):
        self.results = {}
        self.module_error = {}
        self.last_results = self.parse_perf_data(perf_data)
        self.sw = time.Stopwatch()
        self.expected_modules = set([''])
        self.printed_lines = 0
        self.up_to_date = False
        self.perf_mode = perf_mode

    def parse_perf_data(self, perf_data: str) -> dict[str, dict[str, TestInfo]]:
        """Parse performance data from a file
        """
        try:
            pd = json.decode(perf_data)
            res = {}
            for modname, modres in pd.items():
                resmodres = {}
                if isinstance(modname, str) and isinstance(modres, dict):
                    for tname, tinfo in modres.items():
                        resmodres[tname] = TestInfo.from_json(tinfo)
                res[modname] = resmodres
            return res
        except:
            print("Failed to parse performance data")
            return {}

    def set_expected(self, tests: dict[str, Test], modname: str=""):
        for test_name, test in tests.items():
            if modname not in self.results:
                self.results[modname] = {}
            self.results[modname][test_name] = TestInfo(test)

    def update_module(self, module_name: str, test_results: dict[str, TestInfo]):
        """Update test results for all tests in a module
        """
        self.up_to_date = False
        self.results[module_name] = test_results

    def update(self, module_name: str, test_name: str, test_info: TestInfo):
        """Update result for individual test
        """
        self.up_to_date = False
        if module_name not in self.results:
            self.results[module_name] = {}
        self.results[module_name][test_name] = test_info

    def update_result(self, module_name: str, test: Test, complete: bool, test_result: TestResult):
        """Update result for individual test
        """
        self.up_to_date = False
        if module_name not in self.results:
            self.results[module_name] = {}
        if test.name not in self.results[module_name]:
            self.results[module_name][test.name] = TestInfo(test)
        tres = self.results[module_name][test.name]
        if test_result is not None:
            tres.update(complete, test_result)

    def update_module_error(self, module_name: str, moderr: ModuleError):
        self.module_error[module_name] = moderr

    def num_tests(self):
        cnt = 0
        for module_name in self.results:
            cnt += len(self.results[module_name])
        return cnt

    def skip_show(self):
        if len(self.expected_modules) > 0 and set(self.results.keys()) != self.expected_modules:
            return True
        return False

    def is_test_done(self, modname, name):
        if modname in self.results and name in self.results[modname]:
            test_info = self.results[modname][name]
            return test_info.complete
        return False

    def to_json(self):
        """Return JSON encoded results that can be saved to a file
        """
        # TODO: rewrite using comprehensions
        res = {}
        for modname in self.results:
            modres = {}
            for tname, tinfo in self.results[modname].items():
                modres[tname] = tinfo.to_json()
            res[modname] = modres
        return json.encode(res)

    def show(self, only_show_complete=False):
        def format_timing(module_name, test_name):
            def format_diff(new, old):
                diff = new - old
                pct_diff = diff / old * 100
                sign = "+" if diff > 0 else ""
                color = term.green if diff < 0 else term.red
                diff_str = "%s%.2f%%" % (sign, pct_diff)
                return color + "%-10s" % diff_str + term.normal

            tinfo = self.results[module_name][test_name]

            min_dur = "--"
            avg_dur = "--"
            max_dur = "--"
            min_dur_diff = "--"
            avg_dur_diff = "--"
            max_dur_diff = "--"
            if tinfo.num_iterations > 0:
                min_dur = "%5.2f" % tinfo.min_duration + term.grey13 + "ms" + term.normal
                avg_dur = "%5.2f" % tinfo.avg_duration + term.grey13 + "ms" + term.normal
                max_dur = "%5.2f" % tinfo.max_duration + term.grey13 + "ms" + term.normal
                min_dur_diff = ""
                avg_dur_diff = ""
                max_dur_diff = ""

                try:
                    last_tinfo = self.last_results[module_name][test_name]
                    min_dur_diff = format_diff(tinfo.min_duration, last_tinfo.min_duration)
                    avg_dur_diff = format_diff(tinfo.avg_duration, last_tinfo.avg_duration)
                    max_dur_diff = format_diff(tinfo.max_duration, last_tinfo.max_duration)
                except:
                    pass

            return "%7s %10s %sAvg:%s %7s %10s %7s %10s" % (min_dur, min_dur_diff, term.grey18, term.normal, avg_dur, avg_dur_diff, max_dur, max_dur_diff)

        if self.skip_show() or self.up_to_date:
            return
        self.up_to_date = True

        complete = True
        if set(self.results.keys()) != self.expected_modules:
            complete = False
        for module_name in self.results:
            for test_name, test_info in self.results[module_name].items():
                if not test_info.complete:
                    complete = False
        if only_show_complete and not complete:
            return

        errors = 0
        failures = 0

        for i in range(self.printed_lines):
            print(term.clearline + term.up() + term.clearline, end="")
        self.printed_lines = 0
        tname_width = 20
        for modname in self.results:
            for tname, tinfo in self.results[modname].items():
                tname_width = max([tname_width, len(tinfo.definition.name)])
        tname_width += 5

        for modname in self.results:
            if modname == "":
                print("\nTests")
                self.printed_lines += 2
            elif len(self.results[modname]) > 0:
                print("\nTests - module %s:" % modname)
                self.printed_lines += 2
            if modname in self.module_error:
                moderr = self.module_error[modname]
                print("Exit code: %d  term signal: %d" % (moderr.exit_code, moderr.term_signal))
                print(moderr.err)
                continue
            for tname, tinfo in self.results[modname].items():
                prefix = "  " + tinfo.definition.name + ": "
                prefix += " " * (tname_width - len(prefix))
                success = tinfo.success
                exc = tinfo.exception
                status = ""
                msg = ""
                run_info = ""
                if tinfo.complete:
                    if exc is not None:
                        msg += term.bold + term.red
                        if tinfo.flaky:
                            msg += "FLAKY "
                        if tinfo.num_errors > 0:
                            msg += "ERR"
                            errors += 1
                            run_info += "%d errors" % tinfo.num_errors
                        if tinfo.num_errors > 0 and tinfo.num_failures > 0:
                            msg += "/"
                            run_info += " and "
                        if tinfo.num_failures > 0:
                            msg += "FAIL"
                            failures += 1
                            run_info += "%d failures" % tinfo.num_failures
                        if tinfo.num_errors > 0 or tinfo.num_failures > 0:
                            run_info += " out of "
                    else:
                        msg += term.green + "OK" + term.normal
                else:
                    msg = term.yellow + "**" + term.normal

                run_info += "%4d runs in %3.3fms" % (tinfo.num_iterations, tinfo.test_duration)

                l = prefix + msg + ": "
                if self.perf_mode:
                    l += format_timing(modname, tname) + " "
                l += run_info + term.normal
                print(l)
                self.printed_lines += 1
                if tinfo.complete and exc is not None:
                    for line in str(exc).splitlines(None):
                        print(term.red + "    %s" % (line) + term.normal)
                        self.printed_lines += 1
        if complete:
            print("")
            if errors > 0 and failures > 0:
                print(term.bold + term.red + "%d error and %d failure out of %d tests (%ss)" % (errors, failures, self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 2
            elif errors > 0:
                print(term.bold + term.red + "%d out of %d tests errored (%ss)" % (errors, self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 2
            elif failures > 0:
                print(term.bold + term.red + "%d out of %d tests failed (%ss)" % (failures, self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 1
            else:
                print(term.green + "All %d tests passed (%ss)" % (self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 0

class ModuleTestResults(object):
    tests: dict[str, TestInfo]
    last_show: time.Instant

    def __init__(self):
        self.tests = {}
        self.last_show = time.monotonic()
        self.last_show.second -= 1

    def set_tests(self, tests: dict[str, TestInfo]):
        self.tests = tests

    def update(self, test: Test, complete: bool, test_result: TestResult):
        if test.name not in self.tests:
            self.tests[test.name] = TestInfo(test)
        tinfo = self.tests[test.name]
        tinfo.update(complete, test_result)
        self.show_test(test.name)

    def is_test_done(self, name):
        if name in self.tests:
            return self.tests[name].complete
        return False

    def show_test(self, test_name: str):
        tinfo = self.tests[test_name]
        now = time.monotonic()
        if tinfo.complete == False and now.since(self.last_show).to_float() < 0.05:
            return
        self.last_show = now
        print(json.encode({"test_info": tinfo.to_json()}), stderr=True)

    def show(self):
        res = {}
        for test in self.tests.values():
            res[test.definition.name] = test.definition.to_json()
        print(json.encode({"tests": res}), stderr=True)



actor test_runner(env: Env, unit_tests: dict[str, UnitTest], sync_actor_tests: dict[str, SyncActorTest], async_actor_tests: dict[str, AsyncActorTest], env_tests: dict[str, EnvTest]):
    sw = time.Stopwatch()
    var results = ModuleTestResults()

    def _init_results(args):

        all_tests = {}
        for name, t in unit_tests.items():
            all_tests[name] = t
        for name, t in sync_actor_tests.items():
            all_tests[name] = t
        for name, t in async_actor_tests.items():
            all_tests[name] = t
        for name, t in env_tests.items():
            all_tests[name] = t

        tests = _filter_tests(all_tests, args)

        test_module_results = {}
        for test_def in tests.values():
            test_module_results[test_def.name] = TestInfo(test_def)
        results.set_tests(test_module_results)

    def _filter_tests[T](tests: dict[str, T], args) -> dict[str, T]:
        res = {}
        name_filter = []
        try:
            name_filter = args.get_strlist("name")
        except argparse.ArgumentError:
            pass
        test_names = set(name_filter)
        if test_names == set():
            return tests

        TEST_PREFIX_LEN = len("_test_")
        for name, t in tests.items():
            if name[TEST_PREFIX_LEN:] in test_names:
                res[name] = t
        return res

    proc def _list_tests(args):
        _init_results(args)
        tests = []
        for test in results.tests.values():
            tests.append(test.definition.to_json())
        print(json.encode({"tests": tests}), stderr=True)
        env.exit(0)

    proc def _run_tests(args, perf_mode: bool=False):
        trconfig = TestRunnerConfig(perf_mode, args)
        if trconfig.perf_mode:
            acton.rts.start_gc_performance_measurement(env.syscap)
        _init_results(args)
        if trconfig.output_enabled:
            results.show()
        # start with unit tests, proceed with next test category after unit tests
        _run_unit_tests(args, trconfig)

    proc def _run_unit_tests(args, config: TestRunnerConfig):
        test_concurrency = 1 if config.perf_mode else env.nr_wthreads
        my_tests = _filter_tests(unit_tests, args)
        to_hand_out = list(my_tests.keys())
        tests_complete = set()

        def get_test() -> UnitTest:
            test_id = to_hand_out.pop()
            test = my_tests[test_id]
            return test

        def check_complete():
            if tests_complete == set(my_tests.keys()):
                _run_sync_actor_tests(args, config)
                return True
            return False

        def report_complete(t):
            tests_complete.add("_test_" + t)
            check_complete()

        if not check_complete():
            for i in range(test_concurrency):
                unit_test_runner(env.syscap, config, get_test, report_complete)

    proc def _run_sync_actor_tests(args, config: TestRunnerConfig):
        test_concurrency = 1 if config.perf_mode else min([1, env.nr_wthreads // 4])
        my_tests = _filter_tests(sync_actor_tests, args)
        to_hand_out = list(my_tests.keys())
        tests_complete = set()

        def get_test() -> SyncActorTest:
            test_id = to_hand_out.pop()
            test = my_tests[test_id]
            return test

        def check_complete():
            if tests_complete == set(my_tests.keys()):
                _run_async_actor_tests(args, config)
                return True
            return False

        def report_complete(t):
            tests_complete.add("_test_" + t)
            check_complete()

        if not check_complete():
            for i in range(test_concurrency):
                sync_actor_test_runner(env.syscap, config, get_test, report_complete)

    proc def _run_async_actor_tests(args, config: TestRunnerConfig):
        test_concurrency = 1 if config.perf_mode else min([1, env.nr_wthreads // 4])
        my_tests = _filter_tests(async_actor_tests, args)
        to_hand_out = list(my_tests.keys())
        tests_complete = set()

        def get_test() -> AsyncActorTest:
            test_id = to_hand_out.pop()
            test = my_tests[test_id]
            return test

        def check_complete():
            if tests_complete == set(my_tests.keys()):
                _run_env_tests(args, config)
                return True
            return False

        def report_complete(t):
            tests_complete.add("_test_" + t)
            check_complete()

        if not check_complete():
            for i in range(test_concurrency):
                async_actor_test_runner(env.syscap, config, get_test, report_complete)

    proc def _run_env_tests(args, config: TestRunnerConfig):
        test_concurrency = 1 if config.perf_mode else min([1, env.nr_wthreads // 4])
        my_tests = _filter_tests(env_tests, args)
        to_hand_out = list(my_tests.keys())
        tests_complete = set()

        def get_test() -> EnvTest:
            test_id = to_hand_out.pop()
            test = my_tests[test_id]
            return test

        def check_complete():
            if tests_complete == set(my_tests.keys()):
                env.exit(0)
                return True
            return False

        def report_complete(t):
            tests_complete.add("_test_" + t)
            check_complete()

        if not check_complete():
            for i in range(test_concurrency):
                env_test_runner(env.syscap, config, get_test, report_complete, env)


    proc def _run_perf_tests(args):
        _run_tests(args, perf_mode=True)

    def _parse_args():
        p = argparse.Parser()
        p.add_bool("json", "Output results as JSON")
        p.add_bool("no_output", "No result output")
        lp = p.add_cmd("list", "list tests", _list_tests)
        tp = p.add_cmd("test", "Run tests", _run_tests)
        tp.add_option("name", "strlist", nargs="+", default=[], help="Filter tests by name")
        pp = tp.add_cmd("perf", "Performance benchmark tests", _run_perf_tests)
        pp.add_option("iterations", "int", "?", 1, "Number of iterations to run")
        pp.add_option("min_time", "int", "?", 1000, "Minimum time to run a test in milliseconds")

        args = p.parse(env.argv)
        _cmd = args.cmd
        if _cmd is not None:
            _cmd(args)
        else:
            env.exit(0)
    try:
        _parse_args()
    except argparse.PrintUsage as exc:
        print(exc.error_message)
        env.exit(0)
    except argparse.ArgumentError as exc:
        print(exc.error_message)
        env.exit(1)
