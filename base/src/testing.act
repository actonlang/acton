
import acton.rts
import argparse
import file
import json
import logging
import term
import time

# -- assert ---------------------------------------------------------------------

class NotEqualError[T](AssertionError):
    a: ?T
    b: ?T

    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "Expected equal values but they are non-equal."
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += " A: " + str(a) if a is not None else "None"
            b = self.b
            msg += " B: " + str(b) if b is not None else "None"
        return msg

class EqualError[T](AssertionError):
    a: ?T
    b: ?T

    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "Expected non-equal values but they are equal."
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += " A: " + str(a) if a is not None else "None"
            b = self.b
            msg += " B: " + str(b) if b is not None else "None"
        return msg

class NotTrueError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None):
        self.a = a
        self.error_message = msg if msg is not None else "Expected True but got non-True"
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", value: " + str(a) if a is not None else "None"
        return msg

class NotFalseError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None):
        self.a = a
        self.error_message = msg if msg is not None else "Expected False but got non-False."
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", value: " + str(a) if a is not None else "None"
        return msg

class NotNoneError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None):
        self.a = a
        self.error_message = msg if msg is not None else "Expected None but got non-None."
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", value: " + str(a) if a is not None else "None"
        return msg

class NoneError[T](AssertionError):
    a: ?T

    def __init__(self, a, msg: ?str=None):
        self.a = a
        self.error_message = msg if msg is not None else "Expected non-None but got None."
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", value: " + str(a) if a is not None else "None"
        return msg

class NotInError[T,U](AssertionError):
    a: ?T
    b: ?U

    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "Expected element not in container"
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", element: " + str(a) if a is not None else "None"
            b = self.b
            msg += ", container: " + str(b) if b is not None else "None"
        return msg

class InError[T,U](AssertionError):
    a: ?T
    b: ?U

    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "Expected element in container"
        self.print_vals = True if msg is None else False

    def __str__(self):
        msg = "%s: %s" % (self._name(), self.error_message)
        if self.print_vals:
            a = self.a
            msg += ", element: " + str(a) if a is not None else "None"
            b = self.b
            msg += ", container: " + str(b) if b is not None else "None"
        return msg

class NotIsError[T(Identity)](AssertionError):
    a: ?T
    b: ?T
    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "a is not b"

class IsError[T(Identity)](AssertionError):
    a: ?T
    b: ?T
    def __init__(self, a, b, msg: ?str=None):
        self.a = a
        self.b = b
        self.error_message = msg if msg is not None else "a is b"

class NotRaisesError(AssertionError):
    def __init__(self, msg):
        self.error_message = msg
        self.a = None

class IsInstanceError(AssertionError):
    def __init__(self, msg):
        self.error_message = msg
        self.a = None
        self.b = None

class NotIsInstanceError(AssertionError):
    def __init__(self, msg):
        self.error_message = msg
        self.a = None
        self.b = None


def assertEqual[T(Eq)](a: ?T, b: ?T, msg: ?str):
    if ((a is None and b is not None)
        or (a is not None and b is None)
        or (a is not None and b is not None and not (a == b))):
        raise NotEqualError(a, b, msg)

def assertNotEqual[T(Eq)](a: ?T, b: ?T, msg: ?str):
    if ((a is None and b is None)
        or (a is not None and b is not None and a == b)):
        raise EqualError(a, b, msg)

def assertTrue(a, msg: ?str):
    if not bool(a):
        raise NotTrueError(a, msg)

def assertFalse(a, msg: ?str):
    if bool(a):
        raise NotFalseError(a, msg)

# TODO: fix this
#def assertIs[T(Identity)](a: ?T, b: ?T, msg: ?str):
#    if not (a is b):
#        raise NotIsError(a, b, msg)
# TODO: fix this
#def assertIsNot[T(Identity)](a: ?T, b: ?T, msg: ?str):
#    if not (a is not b):
#        raise IsError(a, b, msg)

def assertIsNone(a, msg: ?str):
    if not (a is None):
        raise NotNoneError(a, msg)

def assertIsNotNone(a, msg: ?str):
    if not (a is not None):
        raise NoneError(a, msg)

def assertIn(a, b, msg: ?str):
    if not (a in b):
        raise NotInError(a, b, msg)

def assertNotIn(a, b, msg: ?str):
    if a in b:
        raise InError(a, b, msg)


def error(msg: ?str):
    """Raise a generic test error"""
    raise AssertionError(msg if msg is not None else "Test error")

# -------------------------------------------------------------------------------

def eq_opt[T(Eq)](a: ?T, b: ?T) -> bool:
    return a is not None and b is not None and a == b or a is None and b is None

class TestLogger(logging.Logger):
    pass

class Test(object):
    module: str
    name: str
    desc: str

    def __init__(self, name: str, desc: str, module):
        self.name = name
        self.desc = desc
        self.module = module

    def run(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        if isinstance(self, UnitTest):
            self.run_test(report_result, env, log_handler)
        elif isinstance(self, SyncActorTest):
            self.run_test(report_result, env, log_handler)
        elif isinstance(self, AsyncActorTest):
            self.run_test(report_result, env, log_handler)
        elif isinstance(self, EnvTest):
            self.run_test(report_result, env, log_handler)
        else:
            raise ValueError("Test: Invalid test type")

    def to_json(self):
        return {
            "module": self.module,
            "name": self.name,
            "desc": self.desc,
        }

    @staticmethod
    def from_json(data: dict[str, ?value]):
        module = data["module"]
        name = data["name"]
        desc = data["desc"]
        if isinstance(module, str) and isinstance(name, str) and isinstance(desc, str):
            return Test(name, desc, module)
        raise ValueError("Test: Invalid JSON")

class UnitTest(Test):
    def __init__(self, fn: mut() -> None, name: str, desc: str, module: str):
        self.fn = fn
        self.name = name
        self.desc = desc
        self.module = module

    def run_test(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        output = None
        success = None
        exception = None
        try:
            output = self.fn()
            success = True
            exception = None
        except AssertionError as e:
            success = False
            exception = e
        except Exception as e:
            success = None
            exception = e
        report_result(success, exception, output)

class SyncActorTest(Test):
    def __init__(self, fn: proc(logging.Handler) -> None, name: str, desc: str, module: str):
        self.fn = fn
        self.name = name
        self.desc = desc
        self.module = module

    def run_test(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        output = None
        success = None
        exception = None
        try:
            output = self.fn(log_handler)
            success = True
            exception = None
        except AssertionError as e:
            success = False
            exception = e
        except Exception as e:
            success = None
            exception = e
        report_result(success, exception, output)

class AsyncActorTest(Test):
    def __init__(self, fn: proc(action(?bool, ?Exception) -> None, logging.Handler) -> None, name: str, desc: str, module: str):
        self.fn = fn
        self.name = name
        self.desc = desc
        self.module = module

    def run_test(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        def repres(success: ?bool, exception: ?Exception):
            report_result(success, exception, None)
        self.fn(repres, log_handler)

class EnvTest(Test):
    def __init__(self, fn: proc(action(?bool, ?Exception) -> None, Env, logging.Handler) -> None, name: str, desc: str, module: str):
        self.fn = fn
        self.name = name
        self.desc = desc
        self.module = module

    def run_test(self, report_result: action(?bool, ?Exception, ?str) -> None, env: Env, log_handler: logging.Handler):
        def repres(success: ?bool, exception: ?Exception):
            report_result(success, exception, None)
        self.fn(repres, env, log_handler)


class TestResult(object):
    """
    There are three possible outcomes for a test:
    - success: the test ran to completion with the expected results
      - for unit tests & synchronous actor tests, it means the function returned
      - for asynchronous actor & env tests, the report_result callback was called with TestResult(success=True, exception=None)
    - failure: the test encountered an unexpected value
      - for unit tests & synchronous actor tests, an AssertionError (or child thereof) was raiesd
      - for asynchronous actor & env tests, the report_result callback was called with TestResult(success=False, exception=AssertionError)
    - error: the test was unable to run to completion, encountering some other error in test setup or similar
      - for unit tests & synchronous actor tests, an Exception (or child thereof) was raised, but not an AssertionError
      - for asynchronous actor & env tests, the report_result callback was called with TestResult(success=None, exception=AssertionError)
    """
    success: ?bool
    exception: ?str
    output: ?str
    duration: float
    mem_usage_delta: int
    non_gc_mem_usage_delta: int

    def __init__(self, success: ?bool, exception: ?str, output: ?str, duration: float, mem_usage_delta: int, non_gc_mem_usage_delta: int):
        self.success = success
        self.exception = exception
        self.output = output
        self.duration = duration
        self.mem_usage_delta = mem_usage_delta
        self.non_gc_mem_usage_delta = non_gc_mem_usage_delta

    def to_json(self):
        return {
            "success": self.success,
            "exception": self.exception,
            "output": self.output,
            "duration": self.duration,
            "mem_usage_delta": self.mem_usage_delta,
            "non_gc_mem_usage_delta": self.non_gc_mem_usage_delta,
        }

    @staticmethod
    def from_json(data: dict[str, str]) -> TestResult:
        success = data["success"]
        exception = data["exception"]
        output = data["output"]
        duration = data["duration"]
        mem_usage_delta = data["mem_usage_delta"]
        non_gc_mem_usage_delta = data["non_gc_mem_usage_delta"]
        if (isinstance(success, bool)
            and isinstance(exception, str)
            and isinstance(output, str)
            and isinstance(duration, float)
            and isinstance(mem_usage_delta, int)
            and isinstance(non_gc_mem_usage_delta, int)
            ):
            return TestResult(success, exception, output, duration, mem_usage_delta, non_gc_mem_usage_delta)
        raise ValueError("Invalid TestResult JSON")


class TestInfo(object):
    definition: Test
    complete: bool
    success: ?bool
    exception: ?str
    output: ?str
    flaky_output: bool
    flaky: bool
    leaky: bool
    min_duration: float
    max_duration: float
    avg_duration: float
    total_duration: float
    test_duration: float
    num_iterations: int
    num_failures: int
    num_errors: int
    mem_usage_delta_avg: int
    non_gc_mem_usage_delta_avg: int
    non_gc_mem_inc_count: int
    results: list[TestResult]

    def __init__(self,
                 definition: Test,
                 complete: bool=False,
                 success: ?bool=None,
                 exception: ?str=None,
                 output: ?str=None,
                 flaky: bool=False,
                 min_duration: float=-1.0,
                 max_duration: float=-1.0,
                 avg_duration: float=-1.0,
                 total_duration: float=0.0,
                 test_duration: float=0.0,
                 num_iterations: int=0,
                 num_failures: int=0,
                 num_errors: int=0,
                 mem_usage_delta_avg: int=0,
                 non_gc_mem_usage_delta_avg: int=0,
                 non_gc_mem_inc_count: int=0,
                 results: list[TestResult]=[]):
        self.definition = definition
        self.complete = complete
        self.success = success
        self.exception = exception
        self.output = output
        self.flaky_output = False
        self.flaky = flaky
        self.leaky = False
        self.min_duration = min_duration
        self.max_duration = max_duration
        self.avg_duration = avg_duration
        self.total_duration = total_duration
        self.test_duration = test_duration
        self.num_iterations = num_iterations
        self.num_failures = num_failures
        self.num_errors = num_errors
        self.mem_usage_delta_avg = mem_usage_delta_avg
        self.non_gc_mem_usage_delta_avg = non_gc_mem_usage_delta_avg
        self.non_gc_mem_inc_count = non_gc_mem_inc_count
        self.results = results

    def update(self, complete, result: TestResult, test_duration: float=-1.0):
        self.complete = complete

        if len(self.results) == 0:
            # First result
            self.output = result.output
            self.exception = result.exception

        self.results.append(result)

        if not eq_opt(self.output, result.output):
            self.flaky_output = True

        if test_duration > 0.0:
            self.test_duration = test_duration

        self.flaky = False
        self.leaky = False
        self.min_duration = -1.0
        self.max_duration = -1.0
        self.avg_duration = -1.0
        self.total_duration = 0.0
        self.num_iterations = len(self.results)
        self.num_failures = 0
        self.num_errors = 0
        self.mem_usage_delta_avg = 0
        self.non_gc_mem_usage_delta_avg = 0
        self.non_gc_mem_inc_count = 0
        for result in self.results:
            res_success = result.success
            if res_success == False:
                self.num_failures += 1
            elif res_success is None:
                self.num_errors += 1

        for result in self.results[1:]:
            if result.duration < self.min_duration or self.min_duration < 0.0:
                self.min_duration = result.duration
            if result.duration > self.max_duration or self.max_duration < 0.0:
                self.max_duration = result.duration
            self.total_duration += result.duration

            self.mem_usage_delta_avg += result.mem_usage_delta
            self.non_gc_mem_usage_delta_avg += result.non_gc_mem_usage_delta
            if result.non_gc_mem_usage_delta > 0:
                self.non_gc_mem_inc_count += 1

        if self.num_iterations > 0:
            self.mem_usage_delta_avg = self.mem_usage_delta_avg // self.num_iterations-1
            self.non_gc_mem_usage_delta_avg = self.non_gc_mem_usage_delta_avg // self.num_iterations-1
            self.avg_duration = self.total_duration / float(self.num_iterations)

        if self.non_gc_mem_inc_count > min_def([1, self.num_iterations // 2],1) and self.non_gc_mem_usage_delta_avg > 0:
            self.leaky = True

        if self.num_failures > 0:
            self.success = False
        elif self.num_errors > 0:
            self.success = None
        else:
            self.success = True

        if (self.num_failures == 0 and self.num_errors == 0) or self.num_failures == self.num_iterations or self.num_errors == self.num_iterations:
            self.flaky = False
        else:
            self.flaky = True

    def diff(self, old: ?TestInfo) -> (min_duration: str,
                                       max_duration: str,
                                       avg_duration: str,
                                       mem_usage_delta_avg: str,
                                       non_gc_mem_usage_delta_avg: str,
                                       non_gc_mem_inc_count: str):
        def fmt_diff(new, old, unit="") -> str:
            if old is not None:
                diff = float(new) - float(old)
                pct_diff = diff / float(old) * 100
                sign = "+" if diff > 0 else ""
                color = term.green if diff < 0 else term.red
                diff_str = "%s%.2f%%" % (sign, pct_diff)
                return color + "%-10s" % diff_str + term.normal
            else:
                return "%10s" % ""

        #return (min_duration=fmt_diff(self.min_duration, old.min_duration if old is not None else None, "ms"))
        return (
            fmt_diff(self.min_duration, old.min_duration if old is not None else None, "ms"),
            fmt_diff(self.max_duration, old.max_duration if old is not None else None, "ms"),
            fmt_diff(self.avg_duration, old.avg_duration if old is not None else None, "ms"),
            fmt_diff(self.mem_usage_delta_avg, old.mem_usage_delta_avg if old is not None else None, "B"),
            fmt_diff(self.non_gc_mem_usage_delta_avg, old.non_gc_mem_usage_delta_avg if old is not None else None, "B"),
            fmt_diff(self.non_gc_mem_inc_count, old.non_gc_mem_inc_count if old is not None else None)
        )

    def to_json(self, include_results: bool=False):
        test_results = []
        if include_results:
            for r in self.results:
                test_results.append(r.to_json())

        return {
            "definition": self.definition.to_json(),
            "complete": self.complete,
            "success": self.success,
            "exception": self.exception,
            "output": self.output,
            "flaky": self.flaky,
            "min_duration": self.min_duration,
            "max_duration": self.max_duration,
            "avg_duration": self.avg_duration,
            "total_duration": self.total_duration,
            "test_duration": self.test_duration,
            "num_iterations": self.num_iterations,
            "num_failures": self.num_failures,
            "num_errors": self.num_errors,
            "mem_usage_delta_avg": self.mem_usage_delta_avg,
            "non_gc_mem_usage_delta_avg": self.non_gc_mem_usage_delta_avg,
            "non_gc_mem_inc_count": self.non_gc_mem_inc_count,
            "results": test_results
        }

    @staticmethod
    def from_json(json_data: dict[str, ?value]) -> TestInfo:
        def take_definition(jd: dict[str, ?value]) -> Test:
            if "definition" in jd:
                test_definition = jd["definition"]
                if isinstance(test_definition, dict):
                    return Test.from_json(test_definition)
            raise ValueError("Invalid Test JSON: " + str(jd))
        definition: Test = take_definition(json_data)
        complete = json_data["complete"]
        suc = json_data["success"]
        success: ?bool = None
        if suc is not None and isinstance(suc, bool):
            success = suc
        exc = json_data["exception"]
        exception: ?str = None
        if exc is not None and isinstance(exc, str):
            exception = exc
        out = json_data["output"]
        output: ?str = None
        if out is not None and isinstance(out, str):
            output = out
        flaky = json_data["flaky"]
        min_duration = json_data["min_duration"]
        max_duration = json_data["max_duration"]
        avg_duration = json_data["avg_duration"]
        total_duration = json_data["total_duration"]
        test_duration = json_data["test_duration"]
        num_iterations = json_data["num_iterations"]
        num_failures = json_data["num_failures"]
        num_errors = json_data["num_errors"]
        mem_usage_delta_avg = json_data["mem_usage_delta_avg"]
        non_gc_mem_usage_delta_avg = json_data["non_gc_mem_usage_delta_avg"]
        non_gc_mem_inc_count = json_data["non_gc_mem_inc_count"]
        results: list[TestResult] = []
        # We don't support results in the JSON, since we don't want it for
        # performance reasons but for very generic JSON serialization support it
        # would be correct to support it.
        if (isinstance(complete, bool)
            and isinstance(flaky, bool)
            and isinstance(min_duration, float)
            and isinstance(max_duration, float)
            and isinstance(avg_duration, float)
            and isinstance(total_duration, float)
            and isinstance(test_duration, float)
            and isinstance(num_iterations, int)
            and isinstance(num_failures, int)
            and isinstance(num_errors, int)
            and isinstance(mem_usage_delta_avg, int)
            and isinstance(non_gc_mem_usage_delta_avg, int)
            and isinstance(non_gc_mem_inc_count, int)
            ):
            return TestInfo(definition,
                            complete,
                            success,
                            exception,
                            output,
                            flaky,
                            min_duration,
                            max_duration,
                            avg_duration,
                            total_duration,
                            test_duration,
                            num_iterations,
                            num_failures,
                            num_errors,
                            mem_usage_delta_avg,
                            non_gc_mem_usage_delta_avg,
                            non_gc_mem_inc_count,
                            results)
        else:
            raise ValueError("Invalid TestInfo JSON")

class TestRunnerConfig(object):
    perf_mode: bool
    min_test_duration: float
    output_enabled: bool

    def __init__(self, perf_mode: bool, args):
        self.perf_mode = perf_mode
        mtd = 0.05
        try:
            mtd = float(args.get_int("min_time")) / 1000.0
        except:
            pass
        self.output_enabled = False if args.get_bool("no_output") else True
        self.min_test_duration = mtd


# TODO: add a timeout to this
actor TestExecutor(syscap, config, get_test: () -> Test, report_complete, env):
    """The actual executor of tests
    """
    log_handler = logging.Handler("TestRunner")
    fcap = file.FileCap(env.cap)
    rfcap = file.ReadFileCap(fcap)
    fs = file.FS(fcap)
    var test_sw = time.Stopwatch()
    var last_report = time.Stopwatch()
    var test_info = None

    def get_expected(module: str, test: str) -> ?str:
        filename = file.join_path([fs.cwd(), "test", "golden", module, test])
        try:
            exp_file = file.ReadFile(rfcap, filename)
            exp_data = exp_file.read().decode()
            return exp_data
        except:
            return None

    action def _report_result(test: Test, sw, non_gc_mem_usage_before, gc_total_bytes_start, gc_time_start, success: ?bool, exception: ?Exception, val: ?str):
        full_dur = sw.elapsed().to_float() * 1000.0
        gc_time_end = acton.rts.get_gc_time(syscap).total
        gc_dur = float(gc_time_end - gc_time_start)
        testiter_dur = full_dur - gc_dur
        gc_total_bytes_end = int(acton.rts.get_gc_total_bytes(syscap))
        mem_usage_delta = gc_total_bytes_end - gc_total_bytes_start
        test_dur = test_sw.elapsed().to_float()
        if config.perf_mode:
            acton.rts.gc(syscap)
            acton.rts.gc(syscap)
        non_gc_mem_usage_after = int(acton.rts.get_rss(syscap) - acton.rts.get_mem_usage(syscap))
        non_gc_mem_usage_delta = non_gc_mem_usage_after - non_gc_mem_usage_before
        #print("non-GC memory before: %d  after: %d  delta: %d" % (non_gc_mem_usage_before, non_gc_mem_usage_after, non_gc_mem_usage_delta))
        complete = True if test_dur > config.min_test_duration else False
        if test_info is not None:
            exc = str(exception) if exception is not None else None
            test_info.update(complete, TestResult(success, exc, val, testiter_dur, mem_usage_delta, non_gc_mem_usage_delta), test_dur*1000.0)
        if last_report.elapsed().to_float() > 0.05 or complete:
            if test_info is not None and config.output_enabled:
                print(json.encode({"test_info": test_info.to_json()}), err=True)
            last_report.reset()
        if not complete:
            _run_fn(test)
        else:
            report_complete(test.name)
            _run_next()

    def _run_fn(t: Test):
        # Run GC to get accurate memory usage
        if config.perf_mode:
            acton.rts.gc(syscap)
            acton.rts.gc(syscap)
        non_gc_mem_usage_before = int(acton.rts.get_rss(syscap) - acton.rts.get_mem_usage(syscap))
        gc_total_bytes_start = int(acton.rts.get_gc_total_bytes(syscap))
        gc_time_start = acton.rts.get_gc_time(syscap).total
        sw = time.Stopwatch()

        def repres(s: ?bool, e: ?Exception, val: ?str) -> None:
            # Compare expected golden value
            if val is not None:
                exp_val = get_expected(t.module, t.name)
                if exp_val is None or exp_val is not None and val != exp_val:
                    exc = NotEqualError(val, exp_val, "Test output does not match expected golden value.\nActual  : %s\nExpected: %s" % (val, exp_val if exp_val is not None else "None"))
                    _report_result(t, sw, non_gc_mem_usage_before, gc_total_bytes_start, gc_time_start, False, exc, val)
                    return
            _report_result(t, sw, non_gc_mem_usage_before, gc_total_bytes_start, gc_time_start, s, e, val)

        try:
            t.run(repres, env, log_handler)
        except AssertionError as e:
            _report_result(t, sw, non_gc_mem_usage_before, gc_total_bytes_start, gc_time_start, False, e, None)
        except Exception as e:
            _report_result(t, sw, non_gc_mem_usage_before, gc_total_bytes_start, gc_time_start, None, e, None)

    def _run_next():
        """Get the next available test and run it"""
        test_sw = time.Stopwatch()
        try:
            t = get_test()
            test_info = TestInfo(t)
            _run_fn(t)
        except Exception as e:
            return None

    after 0: _run_next()


class ModuleError(object):
    exit_code: int
    term_signal: int
    errout: str

    def __init__(self, exit_code: int, term_signal: int, err: str):
        self.exit_code = exit_code
        self.term_signal = term_signal
        self.err = err

class ProjectTestResults(object):
    results: dict[str, dict[str, TestInfo]]
    module_error: dict[str, ModuleError]
    last_results: dict[str, dict[str, TestInfo]]
    sw: time.Stopwatch
    expected_modules: set[str]
    printed_lines: int
    up_to_date: bool
    perf_mode: bool

    def __init__(self, perf_data: str, perf_mode: bool=False):
        self.results = {}
        self.module_error = {}
        self.last_results = self.parse_perf_data(perf_data)
        self.sw = time.Stopwatch()
        self.expected_modules = set([''])
        self.printed_lines = 0
        self.up_to_date = False
        self.perf_mode = perf_mode

    def parse_perf_data(self, perf_data: str) -> dict[str, dict[str, TestInfo]]:
        """Parse performance data from a file
        """
        try:
            pd = json.decode(perf_data)
            res = {}
            for modname, modres in pd.items():
                resmodres = {}
                if isinstance(modname, str) and isinstance(modres, dict):
                    for tname, tinfo in modres.items():
                        resmodres[tname] = TestInfo.from_json(tinfo)
                res[modname] = resmodres
            return res
        except:
            print("Failed to parse performance data")
            return {}

    def set_expected(self, tests: dict[str, Test], modname: str=""):
        for test_name, test in tests.items():
            if modname not in self.results:
                self.results[modname] = {}
            self.results[modname][test_name] = TestInfo(test)

    def update_module(self, module_name: str, test_results: dict[str, TestInfo]):
        """Update test results for all tests in a module
        """
        self.up_to_date = False
        self.results[module_name] = test_results

    def update(self, module_name: str, test_name: str, test_info: TestInfo):
        """Update result for individual test
        """
        self.up_to_date = False
        if module_name not in self.results:
            self.results[module_name] = {}
        self.results[module_name][test_name] = test_info

    def update_result(self, module_name: str, test: Test, complete: bool, test_result: TestResult):
        """Update result for individual test
        """
        self.up_to_date = False
        if module_name not in self.results:
            self.results[module_name] = {}
        if test.name not in self.results[module_name]:
            self.results[module_name][test.name] = TestInfo(test)
        tres = self.results[module_name][test.name]
        if test_result is not None:
            tres.update(complete, test_result)

    def update_module_error(self, module_name: str, moderr: ModuleError):
        self.module_error[module_name] = moderr

    def num_tests(self):
        cnt = 0
        for module_name in self.results:
            cnt += len(self.results[module_name])
        return cnt

    def skip_show(self):
        if len(self.expected_modules) > 0 and set(self.results.keys()) != self.expected_modules:
            return True
        return False

    def is_test_done(self, modname, name):
        if modname in self.results and name in self.results[modname]:
            test_info = self.results[modname][name]
            return test_info.complete
        return False

    def is_module_done(self, modname):
        if modname in self.results:
            for tname, test_info in self.results[modname].items():
                if not test_info.complete:
                    return False
        return True

    def to_json(self):
        """Return JSON encoded results that can be saved to a file
        """
        # TODO: rewrite using comprehensions
        res = {}
        for modname in self.results:
            modres = {}
            for tname, tinfo in self.results[modname].items():
                modres[tname] = tinfo.to_json()
            res[modname] = modres
        return json.encode(res)

    def show(self, only_show_complete=False, perf_mem=True):

        def format_diff(new, old, unit=""):
            diff = float(new) - float(old)
            pct_diff = diff / float(old) * 100
            sign = "+" if diff > 0 else ""
            color = term.green if diff < 0 else term.red
            diff_str = "%s%.2f%%" % (sign, pct_diff)
            return color + "%-10s" % diff_str + term.normal

        def format_timing(module_name, test_name):
            tinfo = self.results[module_name][test_name]

            min_dur = "--"
            avg_dur = "--"
            max_dur = "--"
            min_dur_diff = "--"
            avg_dur_diff = "--"
            max_dur_diff = "--"
            if tinfo.num_iterations > 0:
                min_dur = "%6.2f" % tinfo.min_duration + term.grey13 + "ms" + term.normal
                avg_dur = "%6.2f" % tinfo.avg_duration + term.grey13 + "ms" + term.normal
                max_dur = "%6.2f" % tinfo.max_duration + term.grey13 + "ms" + term.normal
                min_dur_diff = ""
                avg_dur_diff = ""
                max_dur_diff = ""

                try:
                    last_tinfo = self.last_results[module_name][test_name]
                    min_dur_diff = format_diff(tinfo.min_duration, last_tinfo.min_duration)
                    avg_dur_diff = format_diff(tinfo.avg_duration, last_tinfo.avg_duration)
                    max_dur_diff = format_diff(tinfo.max_duration, last_tinfo.max_duration)
                except:
                    pass

            return "%8s %10s %sAvg:%s %8s %10s %8s %10s" % (min_dur, min_dur_diff, term.grey18, term.normal, avg_dur, avg_dur_diff, max_dur, max_dur_diff)

        if self.skip_show() or self.up_to_date:
            return
        self.up_to_date = True

        errors = 0
        failures = 0
        complete = True
        if set(self.results.keys()) != self.expected_modules:
            complete = False
        for module_name in self.results:
            if module_name in self.module_error:
                for test_name, test_info in self.results[module_name].items():
                    errors += 1
                continue
            for test_name, test_info in self.results[module_name].items():
                if not test_info.complete:
                    complete = False
        if only_show_complete and not complete:
            return

        for i in range(self.printed_lines):
            print(term.clearline + term.up() + term.clearline, end="")
        self.printed_lines = 0
        tname_width = 20
        for modname in self.results:
            for tname, tinfo in self.results[modname].items():
                tname_width = max_def([tname_width, len(tinfo.definition.name)],0)
        tname_width += 5

        for module_name in sorted(self.results):
            if module_name == "":
                print("\nTests")
                self.printed_lines += 2
            elif len(self.results[module_name]) > 0:
                print("\nTests - module %s:" % module_name)
                self.printed_lines += 2

            for test_name, tinfo in self.results[module_name].items():
                last_tinfo = None
                if module_name in self.last_results and test_name in self.last_results[module_name]:
                    last_tinfo = self.last_results[module_name][test_name]
                prefix = "  " + tinfo.definition.name + ": "
                prefix += " " * (tname_width - len(prefix))
                success = tinfo.success
                exc = tinfo.exception
                status = ""
                msg = ""
                run_info = ""
                if tinfo.complete:
                    if exc is not None:
                        msg += term.bold + term.red
                        if tinfo.flaky:
                            msg += "FLAKY "
                        if tinfo.num_errors > 0:
                            msg += "ERR"
                            errors += 1
                            run_info += "%d errors" % tinfo.num_errors
                        if tinfo.num_errors > 0 and tinfo.num_failures > 0:
                            msg += "/"
                            run_info += " and "
                        if tinfo.num_failures > 0:
                            msg += "FAIL"
                            failures += 1
                            run_info += "%d failures" % tinfo.num_failures
                        if tinfo.num_errors > 0 or tinfo.num_failures > 0:
                            run_info += " out of "
                    else:
                        msg += term.green + "OK" + term.normal
                    msg += ": "
                    if self.perf_mode:
                        msg += format_timing(module_name, test_name) + " "
                    msg += "%4d runs in %3.3fms" % (tinfo.num_iterations, tinfo.test_duration) + term.normal
                elif module_name in self.module_error:
                    moderr = self.module_error[module_name]
                    msg = term.red + "##: Test crash? Exit code: %d  term signal: %d" % (moderr.exit_code, moderr.term_signal) + term.normal
                else:
                    msg = term.yellow + "**" + term.normal

                l = prefix + msg
                print(l)
                self.printed_lines += 1
                if self.perf_mode and perf_mem:
                    indent = " " * 10
                    diffs = tinfo.diff(last_tinfo)
                    print(indent + "Memory usag      : %6dKB %s" % (tinfo.mem_usage_delta_avg // 1024, diffs.mem_usage_delta_avg))
                    print(indent + "non-GC usage     : %6dKB %s" % (tinfo.non_gc_mem_usage_delta_avg // 1024, diffs.non_gc_mem_usage_delta_avg))
                    print(indent + "non-GC usage > 0 : %d%% (%d out of %d)" % ((tinfo.non_gc_mem_inc_count * 100 // tinfo.num_iterations) if tinfo.num_iterations > 0 else 0,
                                                                        tinfo.non_gc_mem_inc_count,
                                                                        tinfo.num_iterations))
                    self.printed_lines += 3
                if tinfo.complete and exc is not None:
                    for line in str(exc).splitlines(None):
                        print(term.red + "    %s" % (line) + term.normal)
                        self.printed_lines += 1

            if module_name in self.module_error:
                moderr = self.module_error[module_name]
                if moderr.err != "":
                    msg = term.red + "Error:\n" + term.normal
                    msg += moderr.err
                    print(msg)
                    self.printed_lines += len(msg.splitlines())

        print("")
        if complete:
            if self.num_tests() == 0:
                print("Nothing to test")
                print()
                return 0
            elif errors > 0 and failures > 0:
                print(term.bold + term.red + "%d error and %d failure out of %d tests (%ss)" % (errors, failures, self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 2
            elif errors > 0:
                print(term.bold + term.red + "%d out of %d tests errored (%ss)" % (errors, self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 2
            elif failures > 0:
                print(term.bold + term.red + "%d out of %d tests failed (%ss)" % (failures, self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 1
            else:
                print(term.green + "All %d tests passed (%ss)" % (self.num_tests(), self.sw.elapsed().str_ms()) + term.normal)
                print()
                return 0
        else:
            print("Running tests...")
            print()
        self.printed_lines += 3

class ModuleTestResults(object):
    tests: dict[str, TestInfo]
    last_show: time.Instant

    def __init__(self):
        self.tests = {}
        self.last_show = time.monotonic()
        self.last_show.second -= 1

    def set_tests(self, tests: dict[str, TestInfo]):
        self.tests = tests

    def update(self, test: Test, complete: bool, test_result: TestResult):
        if test.name not in self.tests:
            self.tests[test.name] = TestInfo(test)
        tinfo = self.tests[test.name]
        tinfo.update(complete, test_result)
        self.show_test(test.name)

    def is_test_done(self, name):
        if name in self.tests:
            return self.tests[name].complete
        return False

    def show_test(self, test_name: str):
        tinfo = self.tests[test_name]
        now = time.monotonic()
        if tinfo.complete == False and now.since(self.last_show).to_float() < 0.05:
            return
        self.last_show = now
        print(json.encode({"test_info": tinfo.to_json()}), err=True)

    def show(self):
        res = {}
        for test_name, test_info in self.tests.items():
            res[test_name] = test_info.to_json()
        print(json.encode({"tests": res}), err=True)



actor test_runner(env: Env,
                  unit_tests: dict[str, UnitTest],
                  sync_actor_tests: dict[str, SyncActorTest],
                  async_actor_tests: dict[str, AsyncActorTest],
                  env_tests: dict[str, EnvTest]):
    sw = time.Stopwatch()
    var results = ModuleTestResults()
    var all_tests = {}

    def _init_results(args):

        for name, t in unit_tests.items():
            all_tests[name] = t
        for name, t in sync_actor_tests.items():
            all_tests[name] = t
        for name, t in async_actor_tests.items():
            all_tests[name] = t
        for name, t in env_tests.items():
            all_tests[name] = t

        tests = _filter_tests(all_tests, args)

        test_module_results = {}
        for test_def in tests.values():
            test_module_results[test_def.name] = TestInfo(test_def)
        results.set_tests(test_module_results)

    def _filter_tests[T](tests: dict[str, T], args) -> dict[str, T]:
        res = {}
        name_filter = []
        try:
            name_filter = args.get_strlist("name")
        except argparse.ArgumentError:
            pass
        test_names = set(name_filter)
        if test_names == set():
            return tests

        TEST_PREFIX_LEN = len("_test_")
        for name, t in tests.items():
            if name[TEST_PREFIX_LEN:] in test_names:
                res[name] = t
        return res

    proc def _list_tests(args):
        _init_results(args)
        results.show()
        env.exit(0)

    proc def _run_tests(args, perf_mode: bool=False):
        config = TestRunnerConfig(perf_mode, args)
        if config.perf_mode:
            acton.rts.start_gc_performance_measurement(env.syscap)
        _init_results(args)

        test_concurrency = 1 if config.perf_mode else env.nr_wthreads
        my_tests = _filter_tests(all_tests, args)
        to_hand_out = list(my_tests.keys())
        tests_complete = set()

        def get_test():
            test_id = to_hand_out.pop()
            test = my_tests[test_id]
            return test

        def check_complete():
            if tests_complete == set(my_tests.keys()):
                env.exit(0)
                return True
            return False

        def report_complete(t):
            tests_complete.add("_test_" + t)
            check_complete()

        test_executors = []
        if not check_complete():
            for i in range(test_concurrency):
                te = TestExecutor(env.syscap, config, get_test, report_complete, env)
                test_executors.append(te)

    proc def _run_perf_tests(args):
        _run_tests(args, perf_mode=True)

    def _parse_args():
        p = argparse.Parser()
        p.add_bool("json", "Output results as JSON")
        p.add_bool("no_output", "No result output")
        p.add_option("name", "strlist", nargs="+", default=[], help="Filter tests by name")
        lp = p.add_cmd("list", "list tests", _list_tests)
        tp = p.add_cmd("test", "Run tests", _run_tests)
        pp = tp.add_cmd("perf", "Performance benchmark tests", _run_perf_tests)
        pp.add_option("iterations", "int", "?", 1, "Number of iterations to run")
        pp.add_option("min_time", "int", "?", 1000, "Minimum time to run a test in milliseconds")

        args = p.parse(env.argv)
        _cmd = args.cmd
        if _cmd is not None:
            _cmd(args)
        else:
            env.exit(0)
    try:
        _parse_args()
    except argparse.PrintUsage as exc:
        print(exc.error_message)
        env.exit(0)
    except argparse.ArgumentError as exc:
        print(exc.error_message)
        env.exit(1)
