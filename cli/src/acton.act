import argparse
import file
import json
import process
import testing
from testing import TestInfo

def clean_buildcache(cap: file.FileCap):
    fs = file.FS(cap)
    cache_dir = fs.homedir() + "/.cache/acton"
    #cache_dir = fs.join(fs.homedir(), ".cache", "acton")
    total_size = 0
    for f in fs.walk(cache_dir):
        total_size += f.size
    if total_size > 15 * 1024 * 1024 * 1024: # 5GB
        print("Build cache (", total_size // (1024*1024), "MB) over 5GB, cleaning it up.")
        fs.rmtree(cache_dir)
        total_size = 0
    if total_size == 0:
        print("Acton build cache is empty: rebuilding Acton base from source, which might take a while...")

actor CompilerRunner(process_cap, env, args, wdir=None):
    def on_stderr(p, data):
        print(data.decode(), err=True, end="")

    def on_stdout(p, data):
        print(data.decode(), end="")

    def on_exit(p, exit_code, term_signal):
        if exit_code != 0:
            print("actonc exited with code: ", exit_code, " terminated with signal:", term_signal)
        await async env.exit(exit_code)

    def on_error(p, error):
        print("Error from process:", error)
        await async env.exit(1)

    clean_buildcache(file.FileCap(env.cap))
    fs = file.FS(file.FileCap(env.cap))
    # We find the path to actonc by looking at the executable path of the
    # current process. Since we are called 'acton', we just add a 'c'.
    cmd = [fs.exepath() + "c"] + args
    p = process.Process(process_cap, cmd, on_stdout, on_stderr, on_exit, on_error, wdir)


actor CompilerBuildRunner(process_cap, env, args, wdir=None, on_success: action() -> None, on_error: action(str) -> None):
    var stdout_buf = b""
    var stderr_buf = b""

    def on_actonc_stderr(p, data):
        stderr_buf += data

    def on_actonc_stdout(p, data):
        stdout_buf += data

    def on_actonc_exit(p, exit_code, term_signal):
        on_success()

    def on_actonc_error(p, error: str):
        on_error(error)

    fs = file.FS(file.FileCap(env.cap))
    # We find the path to actonc by looking at the executable path of the
    # current process. Since we are called 'acton', we just add a 'c'.
    cmd = [fs.exepath() + "c", "build"] + args
    p = process.Process(process_cap, cmd, on_actonc_stdout, on_actonc_stderr, on_actonc_exit, on_actonc_error, wdir)


actor BuildProject(process_cap, env, args):
    """Build the project including dependencies
    """
    # Find dependencies and compile them first
    fs = file.FS(file.FileCap(env.cap))
    deps = []
    try:
        deps = fs.listdir("deps")
    except OSError:
        pass

    cmdargs = ["--deppath", file.join_path([fs.cwd(), "deps"])] + build_cmd_args(args)
    remaining_deps = {}

    def on_build():
        print("Project built successfully")
        env.exit(0)

    def on_build_error(error):
        print("Error building project", error)
        env.exit(1)

    def on_dep_build(name):
        print("Dependency", name, "built successfully")
        try:
            del remaining_deps[name]
        except KeyError:
            pass

        if len(remaining_deps) == 0:
            print("All dependencies built, building project")
            build_project()

    def build_project():
        #cr = CompilerBuildRunner(process_cap, env, cmdargs, None, on_build, on_build_error)
        #CompilerRunner(process_cap, env, cmdargs)
        cr = CompilerRunner(process_cap, env, ["build"] + cmdargs)

    def on_dep_build_error(name, error):
        print("Error building dependency", name, error)

    if len(deps) == 0:
        build_project()
    else:
        print("Building dependencies:")
        for dep in deps:
            print(" -", dep)
            remaining_deps[dep] = True
            cr = CompilerBuildRunner(process_cap, env, cmdargs + ["--keepbuild"], file.join_path(["deps", dep]), lambda: on_dep_build(dep) , lambda x: on_dep_build_error(dep, x))


actor BuildProjectTests(process_cap, env, on_build_success, on_build_failure, dev: bool=True):
    """Build the project test

    This actor builds the project tests using `actonc build --test`. It calls
    on_build_success with a list of the test modules if the build was
    successful, and on_build_failure with the exit code, term signal if the
    build failed.
    """
    _test_modules: list[str] = []
    modtests = {}
    var stdout_buf = b""
    var stderr_buf = b""
    var stdout_tests = False

    fs = file.FS(file.FileCap(env.cap))
    # We find the path to actonc by looking at the executable path of the
    # current process. Since we are called 'acton', we just add a 'c'.
    cmd = [fs.exepath() + "c"] + ["build", "--test"]
    if dev:
        cmd.append("--dev")

    def on_actbuild_stdout(p, data):
        for line in data.decode().splitlines(False):
            if line == "Test executables:":
                stdout_tests = True
            else:
                if stdout_tests:
                    _test_modules.append(line)
                else:
                    print(line)

    def on_actbuild_stderr(p, data):
        stderr_buf += data

    def on_actbuild_exit(p, exit_code, term_signal):
        if exit_code == 0:
            on_build_success(_test_modules)
        else:
            on_build_failure(exit_code, term_signal, stderr_buf)

    def on_actbuild_error(p, error):
        on_build_failure(-999, -999, error)

    clean_buildcache(file.FileCap(env.cap))

    p = process.Process(process_cap, cmd, on_actbuild_stdout, on_actbuild_stderr, on_actbuild_exit, on_actbuild_error)

def parse_json_tests(data):
    tests: dict[str, testing.TestInfo] = {}
    jdata_tests = data["tests"]
    if isinstance(jdata_tests, dict):
        for jdata_test in jdata_tests.values():
            if isinstance(jdata_test, dict):
                name = jdata_test["name"]
                desc = jdata_test["desc"]
                if isinstance(name, str) and isinstance(desc, str):
                    test_def = testing.Test(name, desc)
                    tests[name] = testing.TestInfo(test_def)
                else:
                    raise ValueError("Invalid test list JSON")
    else:
        raise ValueError("Invalid test list JSON")
    return tests

actor RunModuleTest(process_cap, modname, test_cmd, on_json_output, on_test_error):
    var stdout_buf = b""
    var stderr_buf = b""
    var errout = ""
    var likely_json = False

    def on_stdout(p, data: bytes):
        stdout_buf += data

    def on_stderr(p, data):
        if not likely_json:
            if data.startswith(b"{"):
                likely_json = True
        stderr_buf += data
        lines = stderr_buf.splitlines(True)
        stderr_buf = b""
        for line in lines:
            if likely_json:
                if line.endswith(b"\n"):
                    # valid line
                    try:
                        upd = json.decode(line.decode())
                    except ValueError:
                        pass
                    else:
                        on_json_output(upd)
                else:
                    # incomplete line
                    stderr_buf = line
                    break
            if not likely_json:
                errout += line.decode()


    def on_exit(p, exit_code, term_signal):
        if exit_code != 0 or term_signal != 0:
            on_test_error(exit_code, term_signal, stdout_buf.decode() + errout)

    def on_error(p, error):
        on_test_error(-1, -1, error)

    cmd = ["out/bin/.test_" + modname, "--json"] + test_cmd
    p = process.Process(process_cap, cmd, on_stdout, on_stderr, on_exit, on_error)


actor RunTestList(env, args):
    """Print list of module tests

    Will run the project module test binaries with 'list --json' to get all
    module tests, collect the output and print a list of all project tests.
    """
    process_cap = process.ProcessCap(env.cap)
    var _expected_modules: set[str] = set()
    var _module_tests = {}

    def print_module_tests():
        for mn in _module_tests.keys():
            print("Module %s:" % mn)
            for module_test_name in _module_tests[mn]:
                print(" ", module_test_name)
            print()

    def _on_json_output(module_name, update):
        if module_name in _module_tests:
            raise ValueError("Duplicate list of tests from module: " + module_name)
        if isinstance(update, dict):
            _module_tests[module_name] = []
            update_tests = update["tests"]
            if isinstance(update_tests, dict):
                for test in update_tests.values():
                    _module_tests[module_name].append(test["definition"]["name"])
        else:
            raise ValueError("Unexpected JSON data from module test: " + module_name)

        if set(_module_tests.keys()) == _expected_modules:
            print_module_tests()
            env.exit(0)

    def _on_test_error(exit_code, term_signal, stderr_buf):
        pass

    def _run_tests(module_names: list[str]):
        _expected_modules = set(module_names)

        for module_name in module_names:
            t = RunModuleTest(process_cap, module_name, ["list"], lambda x: _on_json_output(module_name, x), _on_test_error)

    def _on_build_failure(exit_code, term_signal, stderr_buf):
        print("Failed to build project tests")
        print("actonc exited with code %d / %d" %(exit_code, term_signal))
        print("stderr:", stderr_buf)
        env.exit(1)

    project_builder = BuildProjectTests(process_cap, env, _run_tests, _on_build_failure)


actor RunTestTest(env: Env, args, perf_mode: bool=False):
    process_cap = process.ProcessCap(env.cap)
    var expected_modules_list: set[str] = set()
    var _module_tests = {}
    var modules_to_test = set()
    var perf_data = "{}"

    test_cmd_args = []
    for name_filter in args.get_strlist("name"):
        test_cmd_args.extend(["--name", name_filter])

    try:
        perf_file = file.ReadFile(file.ReadFileCap(file.FileCap(env.cap)), "perf_data")
        perf_data = perf_file.read().decode()
    except:
        pass
    ptr = testing.ProjectTestResults(perf_data, perf_mode)

    def _periodic_show():
        r = ptr.show(only_show_complete=not env.is_tty())
        if r is not None:
            if args.get_bool("record"):
                perf_wfile = file.WriteFile(file.WriteFileCap(file.FileCap(env.cap)), "perf_data")
                perf_wfile.write(ptr.to_json().encode())
                perf_wfile.close()
            env.exit(r)
        after 0.05: _periodic_show()

    def _on_json_output(module_name, data):
        if isinstance(data, dict):
            if "tests" in data:
                data_tests = data["tests"]
                tests = {}
                for test_name, json_test_info in data_tests.items():
                    if isinstance(json_test_info, dict):
                        tests[test_name] = TestInfo.from_json(json_test_info)

                ptr.update_module(module_name, tests)
                expected_modules_list.discard(module_name)
                if len(expected_modules_list) == 0:
                    _periodic_show()
                    if perf_mode:
                        _run_module_tests()
                    else:
                        # Run all tests in parallel
                        for module_name in modules_to_test:
                            t = RunModuleTest(process_cap, module_name, ["test"] + test_cmd_args, lambda x: _on_json_output(module_name, x), lambda x, y, z: _on_test_error(module_name, x, y, z))

            elif "test_info" in data:
                test_info = TestInfo.from_json(data["test_info"])
                ptr.update(module_name, test_info.definition.name, test_info)
                if ptr.is_module_done(module_name) and perf_mode:
                    _run_module_tests()
        else:
            raise ValueError("Unexpected JSON data from module test: " + module_name)

    def _on_test_error(module_name: str, exit_code: int, term_signal: int, errout: str):
        errmsg = "Module test error, exit_code %d and term signal %d, errout: " % (exit_code, term_signal)
        try:
            errmsg += errout
        except IndexError:
            pass
        if module_name in ptr.results:
            moderr = testing.ModuleError(exit_code, term_signal, errout)
            ptr.update_module_error(module_name, moderr)
        _run_module_tests()

    def _run_tests(module_names: list[str]):
        select_modules = set(args.get_strlist("module"))
        if len(select_modules) > 0:
            for module_name in module_names:
                if module_name in select_modules:
                    modules_to_test.add(module_name)
        else:
            modules_to_test = set(module_names)
        expected_modules_list = set(modules_to_test)
        ptr.expected_modules = set(modules_to_test)
        if len(modules_to_test) == 0:
            print("No tests found")
            env.exit(0)

        # List all tests first, which we can run in parallel. Once we have the
        # list of all tests we can start running them one at a time in sequence.
        for module_name in modules_to_test:
            t = RunModuleTest(process_cap, module_name, ["list"] + test_cmd_args, lambda x: _on_json_output(module_name, x), lambda x, y, z: _on_test_error(module_name, x, y, z))

    def _run_module_tests():
        try:
            module_name = modules_to_test.pop()
            if module_name is not None:
                t = RunModuleTest(process_cap, module_name, ["test", "perf"] + test_cmd_args, lambda x: _on_json_output(module_name, x), lambda x, y, z: _on_test_error(module_name, x, y, z))
            else:
                _periodic_show()
        except:
            pass

    def _on_build_failure(exit_code, term_signal, stderr_buf):
        print("Failed to build project tests")
        print("actonc exited with code %d / %d" %(exit_code, term_signal))
        print("stderr:", stderr_buf)
        env.exit(1)

    # TODO: let dev mode enable for normal test and disable, i.e. use release mode for perf tests
    # However, we have some stability issue due to wonky compilation
    # optimizations so we get crashes. Thus we now always run in dev mode to get
    # actually working code.
    #dev = not perf_mode
    dev = True
    project_builder = BuildProjectTests(process_cap, env, _run_tests, _on_build_failure, dev=dev)

def build_cmd_args(args):
    cmdargs = []
    for argname, arg in args.options.items():
        if argname == "file":
            continue
        if arg.type == "bool":
            if args.get_bool(argname):
                cmdargs.append("--" + argname)
        elif arg.type == "str":
            if args.get_str(argname) != '':
                cmdargs.append("--" + argname)
                cmdargs.append(args.get_str(argname))
        elif arg.type == "int":
            if args.get_int(argname) != 0:
                cmdargs.append("--" + argname)
                cmdargs.append(str(args.get_int(argname)))

    return cmdargs


actor main(env):
    process_cap = process.ProcessCap(env.cap)

    def _compilefile(_file, args):
        cmdargs = build_cmd_args(args)
        cr = CompilerRunner(process_cap, env, [_file] + cmdargs)

    def _cmd_build(args):
        BuildProject(process_cap, env, args)
        #cmdargs = build_cmd_args(args)
        #cr = CompilerRunner(process_cap, env, ["build"] + cmdargs)

    def _cmd_doc(args):
        env.exit(0)

    def _cmd_new(args):
        cr = CompilerRunner(process_cap, env, ["new", args.get_str("projectdir")])
        env.exit(0)

    def _cmd_test(args):
        run_tests = RunTestTest(env, args, perf_mode=False)

    def _cmd_test_perf(args):
        run_tests = RunTestTest(env, args, perf_mode=True)

    def _cmd_list_test(args):
        run_tests = RunTestList(env, args)

    def _cmd_version(args):
        if args.get_bool("full"):
            CompilerRunner(process_cap, env, ["--version"])
        else:
            CompilerRunner(process_cap, env, ["--numeric-version"])

    def _parse_args():
        p = argparse.Parser()
        p.add_bool("always-build", "Always build")
        p.add_bool("parse", "Show parsing result")
        p.add_bool("kinds", "Show results after kind checking")
        p.add_bool("types", "Show inferred expression types")
        p.add_bool("sigs", "Show inferred type signatures")
        p.add_bool("norm", "Show results after syntactic normalization")
        p.add_bool("deact", "Show results after deactorization")
        p.add_bool("cps", "Show results after CPS conversion")
        p.add_bool("llift", "Show results of lambda lifting")
        p.add_bool("hgen", "Show generated .h header")
        p.add_bool("cgen", "Show generated .c code")
        p.add_bool("ccmd", "Show CC / LD command lines")
        p.add_bool("timing", "Show timing information")
        p.add_bool("auto-stub", "Allow automatica stub detection")
        p.add_bool("stub", "Stub (.ty) file generation only")
        p.add_bool("cpedantic", "Pedantic C compilation")
        p.add_bool("quiet", "Be quiet")
        p.add_bool("debug", "Print debug stuff")
        p.add_bool("dev", "Development mode")
        p.add_bool("only-build", "Only perform final build of .c files, do not compile .act files")
        p.add_bool("skip-build", "Skip final build of .c files")
        p.add_option("root", "str", "?", "", "Set root actor")
        p.add_option("tempdir", "str", "?", "", "Directory for temporary build files")
        p.add_option("syspath", "str", "?", "", "syspath")
        p.add_option("target", "str", "?", "", "Target, e.g. x86_64-linux-gnu.2.28")
        p.add_arg("file", ".act file to compile, or .ty to show", False, "?")
        buildp = p.add_cmd("build", "Build", _cmd_build)
        docp = p.add_cmd("doc", "Show documentation", _cmd_doc)
        newp = p.add_cmd("new", "New project", _cmd_new)
        newp.add_arg("projectdir", "Project directory", True, "?")
        testp = p.add_cmd("test", "Test", _cmd_test)
        testp.add_bool("record", "Record test performance results")
        testp.add_option("module", "strlist", "+", [], "Filter on test module name")
        testp.add_option("name", "strlist", "+", [], "Filter on test name")
        testlistp = testp.add_cmd("list", "List tests", _cmd_list_test)
        testperfp = testp.add_cmd("perf", "Perf", _cmd_test_perf)
        version_p = p.add_cmd("version", "Show version", _cmd_version)
        version_p.add_bool("full", "Show full version info")
        return p.parse(env.argv)

    try:
        _args = _parse_args()
        _cmd = _args.cmd
        _file = None
        try:
            _file = _args.get_str("file")
        except argparse.ArgumentError:
            pass
        if _cmd is not None:
            if _file is not None and (_file.endswith(".act") or _file.endswith(".ty")):
                print("Error: cannot specify both a command and an .act/.ty file", err=True)
                await async env.exit(1)
            _cmd(_args)
        else:
            if _file is not None:
                _compilefile(_file, _args)
            else:
                env.exit(0)
    except argparse.PrintUsage as exc:
        print(exc.error_message)
        env.exit(0)
    except argparse.ArgumentError as exc:
        print(exc.error_message)
        env.exit(1)
