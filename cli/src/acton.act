import acton.rts
import argparse
import file
import json
import net
import process
import term
import testing

from buildy import *

import acton_cli.deps as deps
import acton_cli.github as cli_gh

def get_zig_local_cache_dir(file_cap: file.FileCap):
    fs = file.FS(file_cap)
    return file.join_path([fs.homedir(), ".cache", "acton", "zig-local-cache"])

def get_zig_global_cache_dir(file_cap: file.FileCap):
    fs = file.FS(file_cap)
    return file.join_path([fs.homedir(), ".cache", "acton", "zig-global-cache"])

def base_path(file_cap: file.FileCap):
    fs = file.FS(file_cap)
    return fs.exepath()[0:-len("/bin/acton")]

def check_cache_is_empty(cap: file.FileCap, cache_dir: str) -> bool:
    """Quick check if a cache directory is empty by just listing its contents"""
    fs = file.FS(cap)
    try:
        contents = fs.listdir(cache_dir)
        return len(contents) == 0
    except:
        # Directory doesn't exist, so it's effectively empty
        return True

actor CacheSizeChecker(cap: file.FileCap):
    """Background actor to check cache sizes in parallel with compilation"""
    var local_cache_size = 0
    var global_cache_size = 0
    var should_check = False
    var last_global_warn_size = 0
    var last_local_warn_size = 0
    var counter = -1

    def check_caches():
        (should_check, last_global_warn_size, last_local_warn_size, counter) = _should_run_check()

        if not should_check:
            # Skip the expensive cache size checks this time
            return

        try:
            global_cache_size = _check_cache_size(get_zig_global_cache_dir(cap))
        except Exception as e:
            pass

        try:
            local_cache_size = _check_cache_size(get_zig_local_cache_dir(cap))
        except Exception as e:
            pass

    def _should_run_check() -> (bool, int, int, int):
        fs = file.FS(cap)
        state_file = file.join_path([fs.homedir(), ".cache", "acton", "check_state"])
        check_interval = 50
        gb = 1024 * 1024 * 1024

        counter = check_interval
        last_global = 10 * gb  # Start warning at 10GB
        last_local = 10 * gb   # Start warning at 10GB
        try:
            content = file.ReadFile(file.ReadFileCap(cap), state_file).read().decode()
            lines = content.strip().split("\n")
            if len(lines) >= 1:
                counter = int(lines[0])
            if len(lines) >= 2:
                last_global = int(lines[1])
            if len(lines) >= 3:
                last_local = int(lines[2])
        except:
            # First time run or corrupted file - use defaults
            pass

        counter -= 1
        should_check = (counter <= 0)
        if should_check:
            counter = check_interval

        try:
            f = file.WriteFile(file.WriteFileCap(cap), state_file)
            f.write((str(counter) + "\n" + str(last_global) + "\n" + str(last_local)).encode())
            f.close()
        except:
            pass

        return (should_check, last_global, last_local, counter)

    def _check_cache_size(cache_dir: str) -> int:
        fs = file.FS(cap)
        total_size: int = 0
        for f in fs.walk(cache_dir):
            total_size += int(f.size)

        return total_size

    def get_cache_state() -> (int, int, bool, int, int, int):
        return (local_cache_size, global_cache_size, should_check, last_global_warn_size, last_local_warn_size, counter)

    check_caches()

def write_buildzig(file_cap, build_config: BuildConfig):
    fs = file.FS(file_cap)
    # We want predictable deterministic paths. Zig wants relative paths. Get
    # relative path to / and then from there to satisfy both.
    relative_syspath = file.get_relative_path("/", fs.cwd()) + base_path(file_cap)

    def get_build_zig_templates():
        # Read the build.zig template
        build_zig_template = file.ReadFile(
            file.ReadFileCap(file_cap),
            file.join_path([base_path(file_cap), "builder", "build.zig"])).read().decode()
        build_zig_zon_template = file.ReadFile(
            file.ReadFileCap(file_cap),
            file.join_path([base_path(file_cap), "builder", "build.zig.zon"])).read().decode()
        return build_zig_template, build_zig_zon_template

    build_zig_tpl, build_zig_zon_tpl = get_build_zig_templates()
    # Write build.zig
    b_file = file.WriteFile(file.WriteFileCap(file_cap), "build.zig")
    b_file.write(gen_buildzig(build_zig_tpl, build_config).encode())
    # Write build.zig.zon
    bzz_file = file.WriteFile(file.WriteFileCap(file_cap), "build.zig.zon")
    deps_path = file.get_relative_path(file.join_path([fs.homedir(), ".cache", "acton", "deps"]), fs.cwd())

    bzz_file.write(gen_buildzigzon(build_zig_zon_tpl, build_config, relative_syspath, deps_path).encode())
    await async b_file.close()
    await async bzz_file.close()


actor CompilerRunner(process_cap, env, args, wdir=None, on_exit: ?action(int, int, bytes, bytes) -> None=None, on_error: ?action(str) -> None=None, print_output: bool=True, exe="actonc"):
    """Run the actonc compiler

    Will run actonc with the provided command and arguments. It is possible to
    inject callbacks when actonc exits or encounters an error. If no callbacks
    are provided, the default behavior is to exit when actonc does, possibly
    with an error message. Similarly, if actonc encounters an error, it will
    print the error message and exit.
    """
    var std_out_buf = b""
    var std_err_buf = b""
    var std_out_done = False
    var std_err_done = False

    def on_actonc_std_err(p, data: ?bytes):
        if data is not None:
            std_err_buf += data
            if print_output:
                print(data.decode(), end="")
        else:
            std_err_done = True

    def on_actonc_std_out(p, data: ?bytes):
        if data is not None:
            std_out_buf += data
            if print_output:
                print(data.decode(), end="")
        else:
            std_out_done = True

    def on_actonc_exit(p, exit_code, term_signal):
        if on_exit is not None:
            on_exit(exit_code, term_signal, std_out_buf, std_err_buf)
        else:
            if exit_code != 0:
                print("{exe} exited with code: ", exit_code, " terminated with signal:", term_signal)
            env.exit(exit_code)

    def on_actonc_error(p, error: str):
        if on_error is not None:
            on_error(error)
        else:
            print("Error from process:", error)
            env.exit(1)


    fs = file.FS(file.FileCap(env.cap))
    # We find the path to actonc by looking at the executable path of the
    # current process. Since we are called 'acton', we just add a 'c'.
    cmd = [fs.exepath() + ("c" if exe == "actonc" else "")]
    if exe == "actonc":
        cmd += ["+RTS", "-N", "-A64M", "-RTS"]
    cmd += args
    if env.is_tty():
        cmd.append("--tty")
    p = process.Process(process_cap, cmd, on_actonc_std_out, on_actonc_std_err, on_actonc_exit, on_actonc_error, wdir)

    def stop():
        p.stop()

actor BuildProject(process_cap, env, args, on_build_success: action(str) -> None, on_build_failure: action(int, int, str) -> None, build_tests: bool=False, files: list[str]=[]):
    """Build the project including dependencies

    - check for build.act.json, if found, download dependencies
    - build dependencies
    - build
    """
    fcap = file.FileCap(env.cap)
    rfcap = file.ReadFileCap(fcap)
    fs = file.FS(fcap)
    lock_file = None
    remaining_deps = {}
    zig_global_cache_dir = get_zig_global_cache_dir(file.FileCap(env.cap))
    # For multi-file compilation
    files_to_compile = list(files)
    all_outputs = []

    # Quick check if caches are empty at startup (before building starts)
    local_cache_was_empty = check_cache_is_empty(fcap, get_zig_local_cache_dir(fcap))
    global_cache_was_empty = check_cache_is_empty(fcap, get_zig_global_cache_dir(fcap))

    # Start background cache size checking only in interactive sessions
    cache_checker: ?CacheSizeChecker = None
    if env.is_tty():
        cache_checker = CacheSizeChecker(file.FileCap(env.cap))

    def parse_dep_path_overrides(args) -> dict[str, str]:
        overrides = {}
        for dep_arg in args.get_strlist("dep"):
            parts = dep_arg.split("=", 1)
            if len(parts) == 2:
                dep_name, dep_path = parts[0], parts[1]
                # Ensure we have absolute paths
                abs_path = file.resolve_relative_path(fs.cwd(), dep_path)
                overrides[dep_name] = abs_path
            else:
                print("Invalid dep argument: %s" % dep_arg, err=True)
                print("HINT: Use --dep <depname>=<path> to specify a dependency path", err=True)
                await async env.exit(1)
        return overrides

    dep_path_overrides = parse_dep_path_overrides(args)

    var build_config = BuildConfig()

    def _report_cache_warnings():
        if cache_checker is not None:
            (local_size, global_size, did_check, last_global_warn, last_local_warn, counter) = cache_checker.get_cache_state()

            if not did_check:
                return

            gb = 1024 * 1024 * 1024
            growth_threshold = 1 * gb

            need_update = False
            new_global_warn = last_global_warn
            new_local_warn = last_local_warn

            if global_size > last_global_warn + growth_threshold:
                print("WARN: The global cache has grown to %.2fGB" % (global_size / gb), err=True)
                print("HINT: You can clear the cache with: rm -rf %s" % get_zig_global_cache_dir(fcap), err=True)
                print("INFO: Do NOT clear the cache if you are offline or have a slow connection.", err=True)
                print("INFO: The global cache stores downloaded package dependencies as well as", err=True)
                print("INFO: compiled artifacts for common libraries, like libc.", err=True)
                print("", err=True)
                new_global_warn = global_size
                need_update = True

            if local_size > last_local_warn + growth_threshold:
                print("WARN: The local cache has grown to %.2fGB" % (local_size / gb), err=True)
                print("HINT: You can clear the cache with: rm -rf %s" % get_zig_local_cache_dir(fcap), err=True)
                print("", err=True)
                new_local_warn = local_size
                need_update = True

            if need_update:
                try:
                    state_file = file.join_path([fs.homedir(), ".cache", "acton", "check_state"])
                    f = file.WriteFile(file.WriteFileCap(fcap), state_file)
                    f.write((str(counter) + "\n" + str(new_global_warn) + "\n" + str(new_local_warn)).encode())
                    f.close()
                except:
                    pass

    def _on_actonc_exit(exit_code, term_signal, std_out_buf, std_err_buf):
        _report_cache_warnings()
        if exit_code == 0:
            on_build_success(std_out_buf.decode())
        else:
            on_build_failure(exit_code, term_signal, std_out_buf.decode() + std_err_buf.decode())

    def _on_actonc_failure(error: str):
        _report_cache_warnings()
        on_build_failure(-999, -999, error)

    def _compile_next_file(cmd_base):
        if len(files_to_compile) == 0:
            combined_output = "\n".join(all_outputs)
            on_build_success(combined_output)
            return

        def _on_this_file_exit(exit_code, term_signal, std_out_buf, std_err_buf):
            if exit_code == 0:
                all_outputs.append(std_out_buf.decode())
                # Compile next file
                _compile_next_file(cmd_base)
            else:
                on_build_failure(exit_code, term_signal, std_out_buf.decode() + std_err_buf.decode())

        def _on_this_file_failure(error: str):
            on_build_failure(-999, -999, error)

        filename = files_to_compile.pop(0)
        cr = CompilerRunner(
            process_cap,
            env,
            cmd_base + [filename],
            None,
            _on_this_file_exit,
            _on_this_file_failure,
            print_output=not build_tests
        )

    def build_project():
        search_paths = []

        all_deps = deps.get_deps_recursive(fcap, fs.cwd())
        # Add transitive dependencies to write new build.zig(.zon), but don't
        # override existing ones, including local overrides
        for dep_name, dep in all_deps.pkg_deps.items():
            build_config.dependencies.setdefault(dep_name, dep)
        for dep_name, dep in all_deps.zig_deps.items():
            build_config.zig_dependencies.setdefault(dep_name, dep)
        write_buildzig(file.FileCap(env.cap), build_config)

        for dep_name, dep in build_config.dependencies.items():
            dep_hash = dep.hash
            dep_path = dep.path
            if dep_path is not None:
                # TODO: deconstruct and put together to get OS independent path? i.e. flip / to \ on windows
                if len(dep_path) == 0:
                    pass
                elif dep_path[0] == "/":
                    search_paths.append(file.join_path([dep_path, "out", "types"]))
                else:
                    search_paths.append(file.join_path([fs.cwd(), dep_path, "out", "types"]))
            elif dep_hash is not None:
                # For dependencies with hashes, we have previously copied them
                # from the Zig global cache to ~/.cache/acton/deps/
                dep_dirname = dep_name + "-" + dep_hash
                search_paths.append(file.join_path([fs.homedir(), ".cache", "acton", "deps", dep_dirname, "out", "types"]))
            else:
                raise ValueError("Dependency %s has no path or hash" % dep_name)
        search_path_arg = []
        for sp in search_paths:
            search_path_arg.extend(["--searchpath", sp])
        cmdargs = []
        if files == []:
            cmdargs.extend(["build", "--sub"])
        cmdargs.extend(build_cmd_args(args))
        if build_tests:
            cmdargs.append("--test")
        cmd = cmdargs + search_path_arg

        # Handle multiple files or single file/project build
        if files == []:
            # Building entire project
            cr = CompilerRunner(
                process_cap,
                env,
                cmd,
                None,
                _on_actonc_exit,
                _on_actonc_failure,
                print_output=not build_tests
            )
        else:
            _compile_next_file(cmdargs + search_path_arg)

    def _on_dep_actonc_exit(dep_name, exit_code, term_signal, std_out_buf, std_err_buf):
        if exit_code == 0:
            print("Dependency", dep_name, "built successfully")
            try:
                del remaining_deps[dep_name]
            except KeyError:
                pass

            if len(remaining_deps) == 0:
                print("All dependencies built, building main project")
                build_project()
        else:
            print("ERROR: Error building dependency", dep_name)
            for dep in remaining_deps.values():
                dep.stop()
            env.exit(1)


    def on_dep_build_error(name, error):
        print("Error building dependency", name, error)
        env.exit(1)

    def build_deps():
        # Find dependencies and compile them first
        if len(build_config.dependencies) == 0:
            build_project()
        else:
            print("Building dependencies:")
            dep_args = []
            for dep_name, dep_path in dep_path_overrides.items():
                dep_args.extend(["--dep", dep_name + "=" + dep_path])

            for dep_name, dep in build_config.dependencies.items():
                dep_hash = dep.hash
                dep_path = dep.path
                path = ""
                if dep_path is not None:
                    path = dep_path
                elif dep_hash is not None:
                    path = file.join_path([fs.homedir(), ".cache", "acton", "deps", dep_name + "-" + dep_hash])
                else:
                    raise ValueError("Dependency %s has no path or hash" % dep_name)
                print(" -", dep_name)
                cmd = ["build", "--skip-build"] + dep_build_cmd_args(args) + dep_args
                cr = CompilerRunner(
                    process_cap,
                    env,
                    cmd,
                    path,
                    lambda exit_code, term_signal, std_out_buf, std_err_buf: _on_dep_actonc_exit(dep_name, exit_code, term_signal, std_out_buf, std_err_buf),
                    lambda error_msg: on_dep_build_error(dep_name, error_msg),
                    exe="acton"
                )
                remaining_deps[dep_name] = cr

    def check_deps():
        # 1. fetch dependencies (into zig global cache)
        # 2. copy dependencies from zig global cache to ~/.cache/acton/deps/
        # 3. build dependencies
        # 4. build project
        deps_dir = set()
        dirs = [fs.homedir(), ".cache", "acton", "deps"]
        # Create deps directory if it doesn't exist, going one dir component at a time
        for i in range(1, len(dirs)+1):
            try:
                await async fs.mkdir(file.join_path(dirs[0:i]))
            except:
                pass

        try:
            deps_dir = set(fs.listdir(file.join_path([fs.homedir(), ".cache", "acton", "deps"])))
        except OSError:
            pass

        for dep_name, dep in build_config.dependencies.items():
            # Does deps/X exist?
            dep_hash = dep.hash
            dep_path = dep.path
            if dep_path is not None:
                pass
            elif dep_hash is not None:
                dep_dirname = dep_name + "-" + dep_hash
                if dep_dirname not in deps_dir:
                    src = file.join_path([zig_global_cache_dir, "p", dep_hash])
                    dst = file.join_path([fs.homedir(), ".cache", "acton", "deps", dep_dirname])
                    try:
                        await async fs.mkdir(dst)
                    except:
                        pass
                    print("Copying %s dependency from zig global cache to project local build deps, hash: %s" % (dep_name, dep_hash))
                    await async fs.copytree(src, dst)
            else:
                raise ValueError("Dependency %s has no hash or path set" % dep_name)
        build_deps()

    def on_zig_fetch_error(errmsg):
        print(errmsg, err=True)
        env.exit(1)

    def check_for_buildconfig():
        """Check if build.act.json exists

        If there is a build.act.json file, we will read it and check if the
        dependencies also exist locally. If not, we will download them and
        proceed to build the dependencies and then build the project.

        If there is no build.act.json file, we will build the project.
        """
        try:
            build_config = BuildConfig.from_json(file.ReadFile(rfcap, "build.act.json").read().decode())
        except FileNotFoundError:
            pass
        except ValueError as e:
            print("ERROR:", e.error_message)
            env.exit(1)
            return

        build_config_fetchable = BuildConfig()
        for dep_name, dep in build_config.dependencies.items():
            if dep_name in dep_path_overrides:
                dep_path = dep_path_overrides[dep_name]
                # We want predictable deterministic paths. Zig wants relative
                # paths. Get relative path to / and then from there to satisfy
                # both.
                rel_path = file.join_path([file.get_relative_path("/", fs.cwd()), file.get_relative_path(dep_path, "/")])
                dep.path = rel_path
                print("INFO: Using dependency path %s for %s" % (rel_path, dep.name), err=True)
            else:
                build_config_fetchable.dependencies[dep_name] = dep

        for dep_name, dep in build_config.zig_dependencies.items():
            build_config_fetchable.zig_dependencies[dep_name] = dep

        write_buildzig(file.FileCap(env.cap), build_config)
        zfd = ZigFetchDeps(env, build_config_fetchable, lambda x, y: check_deps(), on_zig_fetch_error)

    def get_lock():
        """Try to get the build lock for the project
        """
        try:
            lock_file = file.WriteFile(file.WriteFileCap(fcap), ".acton.lock", lock=True)
            if env.is_tty():
                if global_cache_was_empty:
                    print("INFO: Acton global cache is empty: rebuilding common libraries (e.g. libc) which will take some time...")
                if local_cache_was_empty:
                    print("INFO: Acton local cache is empty: rebuilding Acton base which will take some time...")

            check_for_buildconfig()
        except OSError:
            # Only warn if we are an interactive session, chances are otherwise
            # that we are running as a sub-compiler for building a dependency
            # and there's a risk of racing for diamond shaped dependencies. It's
            # normal though and will resolve itself, once the other sub-compiler
            # is done, it will be our turn and the up-to-date check will run in
            # ~0 time and then we're done.
            # TODO: perhaps we should have a better signal than tty-check though
            if env.is_tty():
                print("Build lock exists, trying again soon...")
            after 0.1: get_lock()

    # and off we go! Start by grabbing the project build lock
    get_lock()


actor RunModuleTest(process_cap, modname, test_cmd, on_json_output, on_test_error):
    var captured_std_out = b""
    var std_err_buf = b""
    var captured_std_err = b""

    def on_std_out(p, data: ?bytes):
        if data is not None:
            captured_std_out += data

    def on_std_err(p, data: ?bytes):
        # TODO: we shouldn't sent test data in-band on std_err - should be separate pipe
        if data is not None:
            std_err_buf += data
            lines = std_err_buf.splitlines(True)
            std_err_buf = b""
            for i, line in enumerate(lines):
                if not line.endswith(b"\n"):
                    std_err_buf = b"".join(lines[i:])
                    break

                # We have a complete line, check if it is valid JSON
                try:
                    upd = json.decode(line.decode())
                    # We have a complete JSON object, pass it to the callback
                    on_json_output(self, upd, captured_std_out, captured_std_err)
                except ValueError:
                    # Not a complete JSON object, append to captured_std_err
                    if line != b"\n":
                        captured_std_err += line

    def on_exit(p, exit_code, term_signal):
        if exit_code != 0 or term_signal != 0:
            on_test_error(exit_code, term_signal, captured_std_out.decode() + captured_std_err.decode())

    def on_error(p, error):
        on_test_error(-1, -1, error)

    # TODO: fs.join()
    cmd = ["out/bin/.test_" + modname, "--json"] + test_cmd
    p = process.Process(process_cap, cmd, on_std_out, on_std_err, on_exit, on_error)

    def stop():
        p.stop()


actor CmdListTest(env, args):
    """Print list of module tests

    Will run the project module test binaries with 'list --json' to get all
    module tests, collect the output and print a list of all project tests.
    """
    process_cap = process.ProcessCap(env.cap)

    def print_module_tests(err, tests: dict[str, dict[str, testing.TestInfo]]={}):
        if err is not None:
            print(err, err=True)
            env.exit(1)
            return

        for module_name, module_tests in tests.items():
            print("Module %s:" % module_name)
            for test_name, test_info in module_tests.items():
                display_name = test_info.definition.display_name()
                if display_name != test_name:
                    print("  %s (%s)" % (display_name, test_name))
                else:
                    print("  %s" % display_name)
            print()
        env.exit(0)

    def _on_build_success(std_out_buf: str):
        test_modules = []
        std_out_tests = False
        for line in std_out_buf.splitlines(False):
            if line.startswith("Test executables:"):
                std_out_tests = True
            else:
                if std_out_tests:
                    test_modules.append(line)
        l = ListTests(process_cap, test_modules, print_module_tests)

    def _on_build_failure(exit_code, term_signal, std_err_buf: str):
        print("Failed to build project tests")
        print("actonc exited with code %d / %d" % (exit_code, term_signal))
        print("std_err:", std_err_buf)
        env.exit(1)

    project_builder = BuildProject(
        process_cap,
        env,
        args,
        _on_build_success,
        _on_build_failure,
        build_tests=True
    )


actor ListTests(process_cap, test_modules: list[str], on_done: action(?str, ?dict[str, dict[str, testing.TestInfo]]) -> None):
    """List all tests in project
    """
    remaining_mods = {}
    tests: dict[str, dict[str, testing.TestInfo]] = {}

    def _on_exit(module_name, p, exit_code, term_signal, std_out_buf, std_err_buf):
        if exit_code == 0:
            tests[module_name] = {}
            try:
                data = json.decode(std_err_buf.decode())
                if isinstance(data, dict):
                    if "tests" in data:
                        for test_name, json_test_info in data["tests"].items():
                            if isinstance(json_test_info, dict):
                                tests[module_name][test_name] = testing.TestInfo.from_json(json_test_info)
                        del remaining_mods[module_name]
                        if len(remaining_mods) == 0:
                            on_done(None, tests)
                    else:
                        on_done("Error listing tests for module %s: no 'tests' key in JSON" % module_name)
            except ValueError as e:
                on_done("Error parsing JSON from module %s: %s input: %s" % (module_name, str(e), std_err_buf.decode()))
        else:
            on_done("Error listing tests for module %s, exited with code %d and term signal %d: %s" % (module_name, exit_code, term_signal, std_err_buf.decode()))

    def _on_error(module_name, p, error):
        err = "ERROR: Error listing tests for module %s: %s" % (module_name, error)
        for op in remaining_mods.values():
            op.stop()
        on_done(err)

    if len(test_modules) == 0:
        on_done(None, {})

    # List all tests by asking each module about its tests. This runs in
    # parallel.
    for module_name in test_modules:
        cmd = [file.join_path(["out", "bin", ".test_" + module_name]), "list", "--json"]
        p = process.RunProcess(
            process_cap,
            cmd,
            lambda p, exit_code, term_signal, std_out_buf, std_err_buf: _on_exit(module_name, p, exit_code, term_signal, std_out_buf, std_err_buf),
            lambda p, error: _on_error(module_name, p, error))
        remaining_mods[module_name] = p


actor CmdTest(env: Env, args, perf_mode: bool=False):
    """Run project tests, i.e. `acton test`
    """
    process_cap = process.ProcessCap(env.cap)
    # TODO: test concurrency should consider the type of test, so unit tests get
    # 1 CPU while actor / env tests get more. This is a simple approximation for
    # a start...
    concurrency = env.nr_wthreads // 2
    var running_tests = {}
    var tests_to_run = []
    var perf_data = r"{}"
    fs = file.FS(file.FileCap(env.cap))

    test_cmd_args = []
    if args.get_int("iter") > 0:
        test_cmd_args += ["--max-iter", str(args.get_int("iter"))]
        test_cmd_args += ["--min-iter", str(args.get_int("iter"))]
        test_cmd_args += ["--max-time", str(10**6)]
        test_cmd_args += ["--min-time", str(1)]
    else:
        test_cmd_args += ["--max-iter", str(args.get_int("max-iter"))]
        test_cmd_args += ["--min-iter", str(args.get_int("min-iter"))]
        test_cmd_args += ["--max-time", str(args.get_int("max-time"))]
        test_cmd_args += ["--min-time", str(args.get_int("min-time"))]
    for name_filter in args.get_strlist("name"):
        test_cmd_args.extend(["--name", name_filter])

    try:
        perf_file = file.ReadFile(file.ReadFileCap(file.FileCap(env.cap)), "perf_data")
        perf_data = perf_file.read().decode()
    except:
        pass
    ptr = testing.ProjectTestResults(perf_data, perf_mode)

    def _periodic_show():
        r = ptr.show(show_ongoing=not env.is_tty(), show_log=args.get_bool("show-log"))
        if r is not None:
            if args.get_bool("record"):
                perf_wfile = file.WriteFile(file.WriteFileCap(file.FileCap(env.cap)), "perf_data")
                perf_wfile.write(ptr.to_json().encode())
                perf_wfile.close()
            if args.get_bool("golden-update"):
                for module_name, tests in ptr.results.items():
                    for test_name, test_info in tests.items():
                        exc = test_info.exception
                        output = test_info.output
                        if exc is not None and output is not None and exc.startswith("testing.NotEqualError: Test output does not match expected golden value"):
                            rpath = ["test", "golden", module_name]
                            filename = file.join_path([fs.cwd()] + rpath + [test_info.definition.display_name()])
                            for idx in range(1, len(rpath)+1):
                                mkdir_path = file.join_path([fs.cwd()] + rpath[0:idx])
                                try:
                                    a = fs.mkdir(mkdir_path)
                                except:
                                    pass
                            golden_file = file.WriteFile(file.WriteFileCap(file.FileCap(env.cap)), filename)
                            golden_file.write(output.encode())
                            await async golden_file.close()

            env.exit(r)
            return
        after 0.05: _periodic_show()

    def _on_json_output(rmt, test, data, std_out_buf, std_err_buf):
        if isinstance(data, dict):
            if "test_info" in data:
                test_info = testing.TestInfo.from_json(data["test_info"])
                test_info.std_out = std_out_buf.decode()
                test_info.std_err = std_err_buf.decode()
                ptr.update(test_info.definition.module, test_info.definition.name, test_info)
                if test_info.complete:
                    if test_info.success is None and test_info.exception == "Test timeout":
                        rmt.stop()
                    _run_test()
        else:
            raise ValueError("Unexpected JSON data from module test: " + test.module)

    def _on_test_error(test: testing.Test, exit_code: int, term_signal: int, errout: str):
        errmsg = "Test error, exit_code %d and term signal %d, errout: " % (exit_code, term_signal)
        errmsg += errout
        # We construct our own TestInfo here since we didn't get one from the
        # test process, because it crashed...
        ptr.update(
            test.module,
            test.name,
            testing.TestInfo(
                test,
                complete=True,
                success=None,
                exception="Test crash, exit_code %d and term signal %d" % (exit_code, term_signal),
                num_iterations=1,
                num_failures=0,
                num_errors=1
            ))
        _run_test()

    def _run_test():
        try:
            test_info = tests_to_run.pop(0)
            cmd = ["test", test_info.definition.name]
            if perf_mode:
                cmd += ["perf"]
            cmd += test_cmd_args
            t = RunModuleTest(
                process_cap,
                test_info.definition.module,
                cmd,
                lambda rmt, x, std_out_buf, std_err_buf: _on_json_output(rmt, test_info.definition, x, std_out_buf, std_err_buf),
                lambda x, y, z: _on_test_error(test_info.definition, x, y, z))
            running_tests[test_info.definition] = t
            if len(running_tests) < concurrency:
                _run_test()
        except IndexError:
            pass
        _periodic_show()

    def _list_tests(built_modules: list[str]):
        list_modules = []
        select_modules = set(args.get_strlist("module"))
        if len(select_modules) > 0:
            for module_name in built_modules:
                if module_name in select_modules:
                    list_modules.append(module_name)
        else:
            list_modules = built_modules

        if len(list_modules) == 0:
            print("No tests found")
            env.exit(0)
            return

        ListTests(process_cap, list_modules, _on_list_output)

    def _on_list_output(error, tests: dict[str, dict[str, testing.TestInfo]]={}):
        if error is not None:
            print("Error when listing tests", error, err=True)
            env.exit(1)
            return

        expected_modules = set()
        name_filter = set(args.get_strlist("name"))
        for module_name in sorted(tests.keys()):
            module_tests = tests[module_name]
            for test_name, test_info in module_tests.items():
                # Check if test matches by either real name or pretty display name
                display_name = test_info.definition.display_name()
                if len(name_filter) == 0 or test_name in name_filter or display_name in name_filter:
                    ptr.update(module_name, test_name, test_info)
                    tests_to_run.append(test_info)
                    expected_modules.add(module_name)
        ptr.expected_modules = expected_modules

        _run_test()

    def _on_build_success(std_out_buf: str):
        print(term.clearline + term.up() + term.clearline, end="")
        test_modules = []
        std_out_tests = False
        for line in std_out_buf.splitlines(False):
            if line.startswith("Test executables:"):
                std_out_tests = True
            else:
                if std_out_tests:
                    test_modules.append(line)
        _list_tests(test_modules)

    def _on_build_failure(exit_code, term_signal, std_err_buf: str):
        print("Failed to build project tests")
        print("actonc exited with code %d / %d" % (exit_code, term_signal))
        print("std_err:", std_err_buf)
        env.exit(1)

    print("Building project tests...")
    project_builder = BuildProject(process_cap, env, args, _on_build_success, _on_build_failure, build_tests=True)

def dep_build_cmd_args(args):
    """Compute command line arguments for building dependencies

    A subset of the command line arguments are relevant for building dependencies
    """
    cmdargs = []
    for argname, arg in args.options.items():
        if argname in {"always-build", "cpu", "dep", "target"}:
            if arg.type == "bool":
                if args.get_bool(argname):
                    cmdargs.append("--" + argname)
            elif arg.type == "str":
                if args.get_str(argname) != '':
                    cmdargs.append("--" + argname)
                    cmdargs.append(args.get_str(argname))
            elif arg.type == "int":
                if args.get_int(argname) != 0:
                    cmdargs.append("--" + argname)
                    cmdargs.append(str(args.get_int(argname)))
    return cmdargs

def build_cmd_args(args):
    cmdargs = []
    cmdargs.extend(["--optimize", args.get_str("optimize")])

    for argname, arg in args.options.items():
        if argname not in {
            "always-build",
            "auto-stub",
            "ccmd",
            "cgen",
            "cps",
            "cpu",
            "dbg-no-lines",
            "db",
            "deact",
            "dep",
            "hgen",
            "kinds",
            "llift",
            "no-threads",
            "norm",
            "only-build",
            "parse",
            "root",
            "sigs",
            "skip-build",
            "stub",
            "syspath",
            "target",
            "tempdir",
            "timing",
            "types",
            "verbose"
            }:
            continue

        if arg.type == "bool":
            if args.get_bool(argname):
                cmdargs.append("--" + argname)
        elif arg.type == "str":
            if args.get_str(argname) != '':
                cmdargs.append("--" + argname)
                cmdargs.append(args.get_str(argname))
        elif arg.type == "int":
            if args.get_int(argname) != 0:
                cmdargs.append("--" + argname)
                cmdargs.append(str(args.get_int(argname)))

    return cmdargs


actor ZigFetchDeps(env, build_config: BuildConfig, on_done: action(int, int) -> None, on_fetch_error: action(str) -> None, print_output: bool=False):
    """Fetch dependencies and Zig dependencies
    """
    pcap = process.ProcessCap(env.cap)
    fs = file.FS(file.FileCap(env.cap))
    zig = file.join_path([base_path(file.FileCap(env.cap)), "zig", "zig"])

    remaining_deps = {}

    dep_processes = {}
    zig_processes = {}
    var total = 0
    var fetched = 0

    def _on_exit(dep: Dependency, p, exit_code, term_signal, std_out_buf, std_err_buf):
        if exit_code == 0:
            fetched_hash: str = std_out_buf.decode().strip()
            if print_output:
                print("Fetched dependency %s with hash %s" % (dep.name, fetched_hash), err=True)

            dep_hash = dep.hash
            if dep_hash is not None:
                if dep_hash != fetched_hash:
                    errmsg = "ERROR: hash mismatch for dependency %s" % dep.name
                    errmsg += "\nDETAIL: fetched hash (%s) differs from configured hash (%s)" % (fetched_hash, dep_hash)
                    errmsg += "\nHINT: Update build.act.json with the correct hash. Prefer immutable URLs, like a ref to an immutable git tag over a mutable git branch."
                    on_fetch_error(errmsg)
                    return

            fetched += 1
            if isinstance(dep, PkgDependency):
                del dep_processes[dep.name]
            elif isinstance(dep, ZigDependency):
                del zig_processes[dep.name]

            if len(dep_processes) == 0 and len(zig_processes) == 0:
                on_done(total, fetched)
        else:
            on_fetch_error(std_err_buf.decode())

    def _on_error(dep, p, error):
        # TODO: collect all errors before calling on_fetch_error?
        on_fetch_error(error)

    zig_global_cache_dir = get_zig_global_cache_dir(file.FileCap(env.cap))
    for dep_name, dep in build_config.dependencies.items():
        total += 1
        dep_path = dep.path
        if dep_path is not None:
            if print_output:
                print("Skip fetching dependency %s that uses local path (%s)" % (dep.name, dep_path))
            continue

        dep_url = dep.url
        dep_hash = dep.hash
        if dep_url is not None:
            if dep_hash is not None:
                dep_dir = file.join_path([zig_global_cache_dir, "p", dep_hash])
                try:
                    s = fs.stat(dep_dir)
                    continue
                except:
                    pass
            else:
                on_fetch_error("dependency %s does not have a hash configured")

            cmd = [zig, "fetch", "--global-cache-dir", zig_global_cache_dir, dep_url]
            p = process.RunProcess(
                pcap,
                cmd,
                lambda p, exit_code, term_signal, std_out_buf, std_err_buf: _on_exit(dep, p, exit_code, term_signal, std_out_buf, std_err_buf),
                lambda p, error: _on_error(dep, p, error))
            dep_processes[dep.name] = p

    for dep_name, dep in build_config.zig_dependencies.items():
        total += 1
        dep_path = dep.path
        if dep_path is not None:
            if print_output:
                print("Skip fetching dependency %s that uses local path (%s)" % (dep.name, dep_path))
            continue

        dep_url = dep.url
        dep_hash = dep.hash
        if dep_url is not None:
            if dep_hash is not None:
                dep_dir = file.join_path([zig_global_cache_dir, "p", dep_hash])
                try:
                    s = fs.stat(dep_dir)
                    continue
                except:
                    pass
            else:
                on_fetch_error("dependency %s does not have a hash configured")

            cmd = [zig, "fetch", "--global-cache-dir", zig_global_cache_dir, dep_url]
            p = process.RunProcess(
                pcap,
                cmd,
                lambda p, exit_code, term_signal, std_out_buf, std_err_buf: _on_exit(dep, p, exit_code, term_signal, std_out_buf, std_err_buf),
                lambda p, error: _on_error(dep, p, error))
            zig_processes[dep.name] = p

    if len(dep_processes) == 0 and len(zig_processes) == 0:
        on_done(total, fetched)


actor CmdFetch(env, args):
    pcap = process.ProcessCap(env.cap)
    fcap = file.FileCap(env.cap)
    rfcap = file.ReadFileCap(fcap)
    fs = file.FS(file.FileCap(env.cap))
    zig = file.join_path([base_path(file.FileCap(env.cap)), "zig", "zig"])

    remaining_deps = {}


    dep_processes = {}
    zig_processes = {}

    def all_done(total, fetched):
        if total == 0:
            print("No dependencies to fetch", err=True)
        else:
            print("All dependencies up to date", err=True)
        env.exit(0)

    def on_zig_fetch_error(errmsg):
        print(errmsg, err=True)
        env.exit(1)

    def _read_build_config():
        try:
            build_config = BuildConfig.from_json(file.ReadFile(rfcap, "build.act.json").read().decode())
            write_buildzig(file.FileCap(env.cap), build_config)
            zfd = ZigFetchDeps(env, build_config, all_done, on_zig_fetch_error, print_output=True)
        except FileNotFoundError:
            print("No build.act.json file found, nothing to do.")
            env.exit(0)
            return
        except ValueError as e:
            print("ERROR:", e.error_message)
            env.exit(1)
            return
    _read_build_config()


actor CmdPkgAdd(env, args):
    """Add a package dependency to the project
    """
    pcap = process.ProcessCap(env.cap)
    fs = file.FS(file.FileCap(env.cap))

    zig = file.join_path([base_path(file.FileCap(env.cap)), "zig", "zig"])

    # What arguments do we need?
    # - the name of the dependency (e.g. "foo", used in the build.act.json config file)
    # - the URL to fetch it from

    # We need to fetch the dependency and get the (content) hash of the fetched
    # dependency and store it in the build.act.json file. If the dependency is
    # already in the build.act.json file, we need to check if the hash is the
    # same as the one we just fetched. If it is, we do nothing. If it is not,

    dep_name = args.get_str("name")
    try:
        zig_safe_name(dep_name)
    except ValueError as e:
        print("ERROR:", e)
        env.exit(1)
    dep_url = args.get_str("url")

    def get_build_config():
        try:
            return BuildConfig.from_json(file.ReadFile(file.ReadFileCap(file.FileCap(env.cap)), "build.act.json").read().decode())
        except FileNotFoundError:
            # Ignore, we create a new file
            pass
        except ValueError as e:
            print("ERROR:", e.error_message)
            await async env.exit(1)
        return BuildConfig()

    build_config = get_build_config()

    def on_exit(p, exit_code, term_signal, std_out_buf, std_err_buf):
        if exit_code == 0:
            dep_hash = std_out_buf.decode().strip()

            if dep_name in build_config.dependencies:
                bc_dep = build_config.dependencies[dep_name]
                if bc_dep.url != dep_url:
                    print("Updated existing dependency", dep_name, "with new URL", dep_url, " (old", bc_dep.url, ")")
                    bc_dep.url = dep_url
                if bc_dep.hash == dep_hash:
                    print("Dependency", dep_name, "is already up to date, hash:", dep_hash)
                else:
                    print("Updated existing dependency", dep_name, "with new hash", dep_hash, " (old", bc_dep.hash, ")")
                    bc_dep.hash = dep_hash
            else:
                # Add the hash
                build_config.dependencies[dep_name] = PkgDependency(dep_name, dep_url, dep_hash)
                print("Added new package dependency", dep_name, "with hash", dep_hash)

            baj_file = file.WriteFile(file.WriteFileCap(file.FileCap(env.cap)), "build.act.json")
            baj_file.write(build_config.to_json().encode()+b"\n")
            await async baj_file.close()

            env.exit(0)
        else:
            print("Error fetching", std_err_buf.decode())
            env.exit(1)

    def on_error(p, error):
        print("Error fetching", error)
        env.exit(1)

    cmd = [zig, "fetch", "--global-cache-dir", get_zig_global_cache_dir(file.FileCap(env.cap)), args.get_str("url")]
    process.RunProcess(
        pcap,
        cmd,
        on_exit,
        on_error)

actor ZigFetch(on_dl: action(?str, ?str) -> None, env, dep_name: str, dep_url: str):
    """zig fetch

    Calls zig fetch to fetch a dependency and get the hash of the fetched dependency

    Args:
        on_dl: action(err: ?str, hash: ?str) - callback when download is done
        env: Env - the environment
        dep_name: str - the name of the dependency
        dep_url: str - the URL to fetch the dependency from
    """
    pcap = process.ProcessCap(env.cap)
    fs = file.FS(file.FileCap(env.cap))

    zig = file.join_path([base_path(file.FileCap(env.cap)), "zig", "zig"])

    def on_exit(p, exit_code, term_signal, std_out_buf, std_err_buf):
        if exit_code == 0:
            dep_hash = std_out_buf.decode().strip()
            on_dl(None, dep_hash)
        else:
            error = "Error fetching: %s" % std_err_buf.decode()
            on_dl(error, None)

    def on_error(p, error):
        on_dl(error, None)

    cmd = [zig, "fetch", "--global-cache-dir", get_zig_global_cache_dir(file.FileCap(env.cap)), dep_url]
    process.RunProcess(
        pcap,
        cmd,
        on_exit,
        on_error)


actor CmdPkgUpgrade(env, args):
    """Upgrade (or downgrade) a package dependency
    """
    tcpc_cap = net.TCPConnectCap(net.TCPCap(net.NetCap(env.cap)))
    pcap = process.ProcessCap(env.cap)
    fs = file.FS(file.FileCap(env.cap))

    zig = file.join_path([base_path(file.FileCap(env.cap)), "zig", "zig"])

    # GAAH, the TLS lib does DNS lookups which interferes with GC as the malloc
    # / free is asymmetric, we malloc (using GC malloc) while the free happens
    # deeper inside and then incorrectly tries to free a GC malloced chunk
    # We should replace with our own DNS lookups, but until then, disable GC for
    # pkg upgrade. Should not be a practical problem to run without GC.
    # TODO: remove this!!!!
    acton.rts.disable_gc(env.syscap)

    # What arguments do we need?
    # - the name of the dependency (e.g. "foo", used in the build.act.json config file)
    # - the URL to fetch it from

    # We need to fetch the dependency and get the (content) hash of the fetched
    # dependency and store it in the build.act.json file. If the dependency is
    # already in the build.act.json file, we need to check if the hash is the
    # same as the one we just fetched. If it is, we do nothing. If it is not,

    def get_build_config():
        try:
            return BuildConfig.from_json(file.ReadFile(file.ReadFileCap(file.FileCap(env.cap)), "build.act.json").read().decode())
        except FileNotFoundError:
            # Ignore, we create a new file
            pass
        except ValueError as e:
            print("ERROR:", e.error_message)
            await async env.exit(1)
        return BuildConfig()

    build_config = get_build_config()

    remaining_fetch_ref = {}
    remaining_zig_fetch = {}
    new_pkg_urls = {}
    new_pkg_hashes = {}

    def _on_zig_fetch(dep_name: str, err: ?str, hash: ?str):
        del remaining_zig_fetch[dep_name]
        if hash is not None:
            new_pkg_hashes[dep_name] = hash
        if err is not None:
            print("Error fetching updated hash for", dep_name, err)
        if len(remaining_zig_fetch) == 0:
            _update_build_config()

    def _zig_fetch():
        for dep_name, new_url in new_pkg_urls.items():
            bc_dep = build_config.dependencies[dep_name]
            if bc_dep.url != new_url:
                remaining_zig_fetch[dep_name] = True
                ZigFetch(lambda x, y: _on_zig_fetch(dep_name, x, y), env, dep_name, new_url)
        if len(remaining_zig_fetch) == 0:
            print("No dependencies to upgrade")
            env.exit(0)

    def _update_build_config():
        updated = False
        for dep_name, dep in build_config.dependencies.items():
            new_url = new_pkg_urls.get(dep_name)
            new_hash = new_pkg_hashes.get(dep_name)
            if new_url is not None and new_hash is not None and (new_url != dep.url or new_hash != dep.hash):
                print("Updated dependency", dep_name, "with\n  - new URL", new_url, "\n  - new hash", new_hash)
                dep.url = new_url
                dep.hash = new_hash
                updated = True

        if updated:
            baj_file = file.WriteFile(file.WriteFileCap(file.FileCap(env.cap)), "build.act.json")
            baj_file.write(build_config.to_json().encode()+b"\n")
            await async baj_file.close()
            print("Wrote changes to build.act.json")
        else:
            print("No changes to build.act.json")
        env.exit(0)

    def _onPkgFetchRef(dep_name: str, err: ?Exception, url: ?str):
        del remaining_fetch_ref[dep_name]
        if url is not None:
            new_pkg_urls[dep_name] = url
        if err is not None:
            print("Error fetching ref for %s:" % dep_name, err.error_message)
        if len(remaining_fetch_ref) == 0:
            _zig_fetch()

    def _go():
        for dep_name, dep in build_config.dependencies.items():
            dep_repo_url = dep.repo_url
            if dep_repo_url is not None:
                if dep_repo_url.startswith("https://github.com"):
                    print(dep_name, "- fetching ref from", dep_repo_url)
                    remaining_fetch_ref[dep_name] = dep
                    cli_gh.FetchRef(lambda x, y: _onPkgFetchRef(dep_name, x, y), url=dep_repo_url, tcpc_cap=tcpc_cap, ref=dep.repo_ref)
                else:
                    print(dep_name, "- skipping upgrade: Unsupported git forge URL for", dep_name, ":", dep_repo_url, "only https://github.com is supported", err=True)
            else:
                print(dep_name, "- skipping upgrade: repo_url not set", err=True)

        if len(remaining_fetch_ref) == 0:
            print("No dependencies to upgrade")
            env.exit(0)
    _go()


actor CmdPkgRemove(env, args):
    """Remove a package dependency from the project
    """
    pcap = process.ProcessCap(env.cap)
    fs = file.FS(file.FileCap(env.cap))

    zig = file.join_path([base_path(file.FileCap(env.cap)), "zig", "zig"])

    dep_name = args.get_str("name")
    try:
        zig_safe_name(dep_name)
    except ValueError as e:
        print("ERROR:", e)
        env.exit(1)

    def get_build_config():
        try:
            return BuildConfig.from_json(file.ReadFile(file.ReadFileCap(file.FileCap(env.cap)), "build.act.json").read().decode())
        except FileNotFoundError:
            # Ignore, we create a new file
            pass
        except ValueError as e:
            print("ERROR:", e.error_message)
            await async env.exit(1)
        return BuildConfig()

    build_config = get_build_config()

    if dep_name in build_config.dependencies:
        print("Removed package dependency", dep_name)
        del build_config.dependencies[dep_name]
    else:
        print("Dependency", dep_name, "not found in build.act.json. Nothing to do.")

    baj_file = file.WriteFile(file.WriteFileCap(file.FileCap(env.cap)), "build.act.json")
    baj_file.write(build_config.to_json().encode()+b"\n")
    await async baj_file.close()

    env.exit(0)


actor CmdZigPkgAdd(env, args):
    """Add a zig package dependency to the project
    """
    pcap = process.ProcessCap(env.cap)
    fs = file.FS(file.FileCap(env.cap))

    zig = file.join_path([base_path(file.FileCap(env.cap)), "zig", "zig"])

    # What arguments do we need?
    # - the name of the dependency (e.g. "foo", used in the build.act.json config file)
    # - the URL to fetch it from

    # We need to fetch the dependency and get the (content) hash of the fetched
    # dependency and store it in the build.act.json file. If the dependency is
    # already in the build.act.json file, we need to check if the hash is the
    # same as the one we just fetched. If it is, we do nothing. If it is not,

    dep_name = args.get_str("name")
    try:
        zig_safe_name(dep_name)
    except ValueError as e:
        print("ERROR:", e)
        env.exit(1)
    dep_url = args.get_str("url")
    dep_artifacts = args.get_strlist("artifact")

    def get_build_config():
        try:
            return BuildConfig.from_json(file.ReadFile(file.ReadFileCap(file.FileCap(env.cap)), "build.act.json").read().decode())
        except FileNotFoundError:
            # Ignore, we create a new file
            pass
        except ValueError as e:
            print("ERROR:", e.error_message)
            await async env.exit(1)
        return BuildConfig()

    build_config = get_build_config()

    def on_exit(p, exit_code, term_signal, std_out_buf, std_err_buf):
        if exit_code == 0:
            dep_hash = std_out_buf.decode().strip()

            if dep_name in build_config.zig_dependencies:
                if build_config.zig_dependencies[dep_name].url != dep_url:
                    print("Updated existing dependency", dep_name, "with new URL", dep_url, " (old", build_config.zig_dependencies[dep_name].url, ")")
                    build_config.zig_dependencies[dep_name].url = dep_url

                if build_config.zig_dependencies[dep_name].hash == dep_hash:
                    print("Dependency", dep_name, "is already up to date, hash:", dep_hash)
                else:
                    print("Updated existing dependency", dep_name, "with new hash", dep_hash, " (old", build_config.zig_dependencies[dep_name].hash, ")")
                    build_config.zig_dependencies[dep_name].hash = dep_hash
            else:
                # Add the hash
                build_config.zig_dependencies[dep_name] = ZigDependency(dep_name, dep_url, dep_hash, None, {}, dep_artifacts)
                print("Added new Zig package dependency", dep_name, "with hash", dep_hash)

            baj_file = file.WriteFile(file.WriteFileCap(file.FileCap(env.cap)), "build.act.json")
            baj_file.write(build_config.to_json().encode()+b"\n")
            await async baj_file.close()

            env.exit(0)
        else:
            print("Error fetching", std_err_buf.decode())
            env.exit(1)

    def on_error(p, error):
        print("Error fetching", error)
        env.exit(1)

    cmd = [zig, "fetch", "--global-cache-dir", get_zig_global_cache_dir(file.FileCap(env.cap)), args.get_str("url")]
    process.RunProcess(
        pcap,
        cmd,
        on_exit,
        on_error)


actor CmdZigPkgRemove(env, args):
    """Remove a package dependency from the project
    """
    pcap = process.ProcessCap(env.cap)
    fs = file.FS(file.FileCap(env.cap))

    zig = file.join_path([base_path(file.FileCap(env.cap)), "zig", "zig"])

    dep_name = args.get_str("name")
    try:
        zig_safe_name(dep_name)
    except ValueError as e:
        print("ERROR:", e)
        env.exit(1)

    def _get_build_config():
        try:
            build_config = BuildConfig.from_json(file.ReadFile(file.ReadFileCap(file.FileCap(env.cap)), "build.act.json").read().decode())
            _remove_zig_pkg(build_config, dep_name)
        except FileNotFoundError:
            # Ignore, we create a new file
            print("No build.act.json file found, nothing to do.")
            env.exit(0)
        except ValueError as e:
            print("ERROR:", e.error_message)
            env.exit(1)

    def _remove_zig_pkg(build_config, dep_name):
        if dep_name in build_config.zig_dependencies:
            print("Removed Zig package dependency", dep_name)
            del build_config.zig_dependencies[dep_name]
        else:
            print("Zig dependency", dep_name, "not found in build.act.json. Nothing to do.")

        baj_file = file.WriteFile(file.WriteFileCap(file.FileCap(env.cap)), "build.act.json")
        baj_file.write(build_config.to_json().encode()+b"\n")
        await async baj_file.close()

        env.exit(0)

    _get_build_config()


actor main(env):
    file_cap = file.FileCap(env.cap)
    process_cap = process.ProcessCap(env.cap)

    def in_project(filename: str):
        """Are we in a project?
        """
        fs = file.FS(file_cap)
        project_files = ["Acton.toml", "Build.act", "build.act.json"]
        for project_file in project_files:
            try:
                s = fs.stat(project_file)
                # Also check if src/ directory exists next to the project file
                try:
                    src_stat = fs.stat("src")
                    if src_stat.is_dir():
                        return True
                except:
                    continue
            except:
                continue
        return False

    def _compilefile(_file, args):
        if in_project(_file):
            def on_build_success(std_out_buf):
                env.exit(0)

            def on_build_failure(exit_code: int,  term_signal: int, std_err_buf: str):
                env.exit(1)

            BuildProject(process_cap, env, args, on_build_success, on_build_failure, build_tests=False, files=[_file])
        else:
            cmdargs = build_cmd_args(args)
            cr = CompilerRunner(process_cap, env, [_file] + cmdargs)

    def _cmd_build(args):
        def on_build_success(std_out_buf):
            env.exit(0)

        def on_build_failure(exit_code: int,  term_signal: int, std_err_buf: str):
            env.exit(1)

        files = args.get_strlist("files")

        BuildProject(process_cap, env, args, on_build_success, on_build_failure, build_tests=False, files=files)

    def _cmd_doc(args):
        """Run actonc doc command"""
        cmdargs = ["doc"]

        # Optional file argument
        try:
            file_arg = args.get_str("docfile")
            if file_arg != "":
                cmdargs.append(file_arg)
        except argparse.ArgumentError:
            pass  # docfile is optional, ignore if not provided (and we get exception)

        # Format flags
        if args.get_bool("terminal"):
            cmdargs.append("-t")
        elif args.get_bool("md"):
            cmdargs.append("--md")
        elif args.get_bool("html"):
            cmdargs.append("--html")

        # Output file
        output_file = args.get_str("output")
        if output_file != "":
            cmdargs.extend(["-o", output_file])

        # Force color based on TTY (since stdio is piped)
        if env.is_tty():
            cmdargs.extend(["--color", "always"])
        else:
            cmdargs.extend(["--color", "never"])

        # Custom exit handler (suppress CompilerRunner's default message)
        def on_doc_exit(exit_code: int, term_signal: int, std_out_buf: bytes, std_err_buf: bytes):
            env.exit(exit_code)

        CompilerRunner(process_cap, env, cmdargs, on_exit=on_doc_exit)

    def _cmd_fetch(args):
        c = CmdFetch(env, args)

    def _cmd_pkg(args):
        env.exit(0)

    def _cmd_pkg_add(args):
        c = CmdPkgAdd(env, args)

    def _cmd_pkg_upgrade(args):
        c = CmdPkgUpgrade(env, args)

    def _cmd_pkg_rm(args):
        c = CmdPkgRemove(env, args)

    def _cmd_new(args):
        cr = CompilerRunner(process_cap, env, ["new", args.get_str("projectdir")])
        env.exit(0)

    def _cmd_test(args):
        run_tests = CmdTest(env, args, perf_mode=False)

    def _cmd_test_perf(args):
        run_tests = CmdTest(env, args, perf_mode=True)

    def _cmd_list_test(args):
        run_tests = CmdListTest(env, args)

    def _cmd_version(args):
        CompilerRunner(process_cap, env, ["version"])

    def _cmd_zigpkg(args):
        env.exit(0)

    def _cmd_zigpkg_add(args):
        c = CmdZigPkgAdd(env, args)

    def _cmd_zigpkg_rm(args):
        c = CmdZigPkgRemove(env, args)

    def _parse_args():
        p = argparse.Parser()
        p.add_bool("always-build", "Always build")
        p.add_bool("db", "Enable database backend")
        p.add_bool("parse", "Show parsing result")
        p.add_bool("kinds", "Show results after kind checking")
        p.add_bool("types", "Show inferred expression types")
        p.add_bool("sigs", "Show inferred type signatures")
        p.add_bool("norm", "Show results after syntactic normalization")
        p.add_bool("deact", "Show results after deactorization")
        p.add_bool("cps", "Show results after CPS conversion")
        p.add_bool("llift", "Show results of lambda lifting")
        p.add_bool("hgen", "Show generated .h header")
        p.add_bool("cgen", "Show generated .c code")
        p.add_bool("ccmd", "Show CC / LD command lines")
        p.add_bool("timing", "Show timing information")
        p.add_bool("cpedantic", "Pedantic C compilation")
        p.add_bool("dbg-no-lines", "Disable emission of C #line directives (for debugging)")
        p.add_bool("quiet", "Be quiet")
        p.add_bool("verbose", "Verbose output")
        p.add_option("optimize", "str", "?", "Debug", "Optimization mode (Debug, ReleaseSafe, ReleaseSmall, ReleaseFast)")
        p.add_bool("no-threads", "Don't use threads")
        p.add_bool("only-build", "Only perform final build of .c files, do not compile .act files")
        p.add_bool("skip-build", "Skip final build of .c files")
        p.add_bool("tty", "Controlling terminal is interactive")
        p.add_option("jobs", "int", "?", 0, "Max parallel compiler jobs (0 = auto)")
        p.add_option("root", "str", "?", "", "Set root actor")
        p.add_option("tempdir", "str", "?", "", "Directory for temporary build files")
        p.add_option("syspath", "str", "?", "", "syspath")
        p.add_option("target", "str", "?", "", "Target, e.g. x86_64-linux-gnu.2.28")
        p.add_option("cpu", "str", "?", "", "CPU, e.g. x86_64")
        p.add_option("dep", "strlist", "+", [], "Override path to dependency, e.g. --dep-path foo=../my-foo-repo")
        p.add_arg("file", ".act file to compile, or .ty to show", False, "?")

        p_build = p.add_cmd("build", "Build", _cmd_build)
        p_build.add_arg("files", "Specific .act file(s) to build", required=False, nargs="+")

        p_fetch = p.add_cmd("fetch", "Fetch all the things for offline work", _cmd_fetch)

        p_pkg = p.add_cmd("pkg", "Manage package dependencies", _cmd_pkg)

        p_pkg_add = p_pkg.add_cmd("add", "Add package dependency", _cmd_pkg_add)
        p_pkg_add.add_arg("url", "URL of dependency", True, "?")
        p_pkg_add.add_arg("name", "Name of dependency", True, "?")
        p_pkg_add.add_option("hash", "str", "?", "", "Hash of dependency")

        p_pkg_rm = p_pkg.add_cmd("remove", "Remove package dependency", _cmd_pkg_rm)
        p_pkg_rm.add_arg("name", "Name of dependency", True, "?")

        p_pkg_upgrade = p_pkg.add_cmd("upgrade", "Upgrade (or downgrade) package dependency", _cmd_pkg_upgrade)
        #p_pkg_upgrade.add_option("name", "str", "?", help="Dependency name")

        docp = p.add_cmd("doc", "Show documentation", _cmd_doc)
        docp.add_arg("docfile", "Input file (.act or .ty) - optional in projects", False, "?")
        docp.add_bool("terminal", "Terminal output (ASCII format)", short="t")
        docp.add_bool("md", "Output in Markdown format")
        docp.add_bool("html", "Output in HTML format")
        docp.add_option("output", "str", "?", "", "Output file (default: stdout)")

        newp = p.add_cmd("new", "New project", _cmd_new)
        newp.add_arg("projectdir", "Project directory", True, "?")

        testp = p.add_cmd("test", "Test", _cmd_test)
        testp.add_bool("show-log", "Show test log output")
        testp.add_bool("record", "Record test performance results")
        testp.add_bool("golden-update", "Update expected golden values based on current values")
        testp.add_option("iter", "int", default=-1, help="Number of iterations to run a test")
        testp.add_option("max-iter", "int", default=10**6, help="Maximum number of iterations to run a test")
        testp.add_option("min-iter", "int", default=3, help="Minimum number of iterations to run a test")
        testp.add_option("max-time", "int", default=10**3, help="Maximum time to run a test")
        testp.add_option("min-time", "int", default=50, help="Minimum time to run a test (default: 50ms & 1000ms for perf testing)")
        testp.add_option("module", "strlist", "+", [], "Filter on test module name")
        testp.add_option("name", "strlist", "+", [], "Filter on test name")

        testlistp = testp.add_cmd("list", "List tests", _cmd_list_test)

        testperfp = testp.add_cmd("perf", "Perf", _cmd_test_perf)

        version_p = p.add_cmd("version", "Show version", _cmd_version)

        p_zigpkg = p.add_cmd("zig-pkg", "Manage Zig package dependencies", _cmd_zigpkg)

        p_zigpkg_add = p_zigpkg.add_cmd("add", "Add Zig package dependency", _cmd_zigpkg_add)
        p_zigpkg_add.add_arg("url", "URL of dependency", True, "?")
        p_zigpkg_add.add_arg("name", "Name of dependency", True, "?")
        p_zigpkg_add.add_option("hash", "str", "?", "", "Hash of dependency")
        p_zigpkg_add.add_option("artifact", "strlist", "+", [], "Library artifact to link with")

        p_zigpkg_rm = p_zigpkg.add_cmd("remove", "Remove Zig package dependency", _cmd_zigpkg_rm)
        p_zigpkg_rm.add_arg("name", "Name of dependency", True, "?")

        return p.parse(env.argv)

    try:
        _args = _parse_args()
        _cmd = _args.cmd
        _file = None
        try:
            _file = _args.get_str("file")
        except argparse.ArgumentError:
            pass
        if _cmd is not None:
            if _file is not None and (_file.endswith(".act") or _file.endswith(".ty")):
                print("Error: cannot specify both a command and an .act/.ty file", err=True)
                await async env.exit(1)
            _cmd(_args)
        else:
            if _file is not None:
                _compilefile(_file, _args)
            else:
                env.exit(0)
    except argparse.PrintUsage as exc:
        print(exc.error_message)
        env.exit(0)
    except argparse.ArgumentError as exc:
        print(exc.error_message)
        env.exit(1)
